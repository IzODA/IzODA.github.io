{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to IzODA's Documentation and Guides\n\n\n\nSections are organized by components (Apache Spark, Optimized Data Layer/MDS, Anaconda, and Ecosystem). Documentation, Guides, and Demos can be found within their corresponding sections. Thank you for using Open Data Analytics on z/OS!",
            "title": "About"
        },
        {
            "location": "/spark/spark/",
            "text": "Apache Spark\n\n\n\nApache Spark\n is a general purpose, high performance clustering analytics engine that allocates resources and distributes work across a set of processes and tasks. It is organized as a set core functions that underpin a collection of functional libraries.\n\n\n\n\nSpark was originally released through the IBM z/OS Platform for Apache Spark offering in March of 2016.\n\n\nSpark and Anaconda\n\n\nApache Spark has no dependencies on Anaconda. However, Anaconda's package management capabilities have allowed for all of Spark's dependencies to be included in the IzODA Anaconda package library. This means that IzODA is a self-contained offering with no pre-requisites other than z/OS and Java at the proper service levels. It also means that Spark and Anaconda are effectively co-requisites, and should always be installed along with the ODL (MDS) FMID.\n\n\n\n\nAlong with Spark's pre-requisites, those parts of the Jupyter ecosystem that run on z/OS can now be installed via conda from the \nJKG2AT package.\n\n\nInstallation and Customization\n\n\nRefer to the \nIBM Knowledge Center\n for installation and customization instructions. The guides can be downloaded via \nPDF or TOC here.\n Further instructions about how to install and customize Apache Spark are in chapters 2 and 3 of the \nIzODA Installation and Customization Guide.\n\n\nVerify Your Installation\n\n\nWe have created a collection of small sample applications which you can run to verify that Spark is installed and configured properly, and works with the other components of IzODA. Please choose one of the Install Verification Programs (IVPs) from the \nIBM Knowledge Center.\n\n\nReference Material\n\n\nA library of documentation is available for the IBM Open Data Analytics for z/OS offering. This can be found at the \nIBM Knowledge Center.\n\n\nThe \nApache Spark Implementation on IBM z/OS\n is another good source of information about both Spark and ODL.",
            "title": "Overview"
        },
        {
            "location": "/spark/spark/#spark-and-anaconda",
            "text": "Apache Spark has no dependencies on Anaconda. However, Anaconda's package management capabilities have allowed for all of Spark's dependencies to be included in the IzODA Anaconda package library. This means that IzODA is a self-contained offering with no pre-requisites other than z/OS and Java at the proper service levels. It also means that Spark and Anaconda are effectively co-requisites, and should always be installed along with the ODL (MDS) FMID.   Along with Spark's pre-requisites, those parts of the Jupyter ecosystem that run on z/OS can now be installed via conda from the  JKG2AT package.",
            "title": "Spark and Anaconda"
        },
        {
            "location": "/spark/spark/#installation-and-customization",
            "text": "Refer to the  IBM Knowledge Center  for installation and customization instructions. The guides can be downloaded via  PDF or TOC here.  Further instructions about how to install and customize Apache Spark are in chapters 2 and 3 of the  IzODA Installation and Customization Guide.",
            "title": "Installation and Customization"
        },
        {
            "location": "/spark/spark/#verify-your-installation",
            "text": "We have created a collection of small sample applications which you can run to verify that Spark is installed and configured properly, and works with the other components of IzODA. Please choose one of the Install Verification Programs (IVPs) from the  IBM Knowledge Center.",
            "title": "Verify Your Installation"
        },
        {
            "location": "/spark/spark/#reference-material",
            "text": "A library of documentation is available for the IBM Open Data Analytics for z/OS offering. This can be found at the  IBM Knowledge Center.  The  Apache Spark Implementation on IBM z/OS  is another good source of information about both Spark and ODL.",
            "title": "Reference Material"
        },
        {
            "location": "/spark/jupyter-remote-notebook/",
            "text": "Jupyter with Remote Notebook on the Mainframe\n\n\n\nWhen \nSpark 2.0\n was released to the public, it presented a major problem for people who use it in the mainframe environment. Spark 2.0 fully incorporates \nProject Tungsten\n, a key initiative that focuses on improving memory and CPU efficiency. One of the main features of Project Tungsten is to operate directly against binary data in order to eliminate the overhead of JVM objects and garbage collection. This new feature, however, introduced byte-order dependencies and caused issues to Spark applications running in a mixed-endian environment. For example, a Spark driver running inside a \nJupyter Notebook\n on x86 (little-endian) talking to a Spark cluster running on z/OS (big-endian) would encounter some nasty data integrity problem.\n\n\nThe easiest solution to this mixed-endian data integrity problem is to avoid running your Spark driver on a little-endian platform. But there are many compelling reasons to have a remote application - security, scalability, accessibility to name a few. Luckily the Jupyter Notebook extension, \nnb2kg\n, discussed below provides a solution that circumvents the byte-order issue even in a mixed-endian environment.\n\n\nJupyter Kernel Gateway on the Mainframe\n\n\nWhat is \nJupyter Kernel Gateway?\n Jupyter Kernel Gateway has several capabilities, but one of these capabilities is to offer Jupyter Notebook kernels to remote hosts. The kernels will run on the mainframe but can be executed by an x86 system.\n\n\n\n\nThe diagram above shows how this process works. An end user would connect to a Jupyter Notebook Server with the nb2kg extension, in this case running as a \nDocker\n container. The nb2kg extension would connect to a Jupyter Kernel Gateway Server and would request access to the kernels it hosts. The Jupyter Kernel Gateway Server, which in this case runs on z/OS, would offer the kernels it hosts to the Jupyter Notebook Server that requested them, and then would run any notebooks on platform. In this reference architecture, one of the kernels the Jupyter Kernel Gateway Server would be offering is the Apache Toree kernel which offers the user an Apache Spark, Scala, Notebook environment to run under. Because the Apache Toree kernel is running on z/OS, there are no byte order issues.\n\n\nSetup Jupyter Notebook to Kernel Gateway w/ Apache Toree Environment\n\n\nSetup the Kernel Gateway and Apache Toree on z/OS\n\n\nGiven that this example revolves around the mainframe, a lot of the work will be done on a z/OS host. To install and run the Jupyter Kernel Gateway with Apache Toree solution on z/OS follow these \ninstructions.\n\n\nSetup the Jupyter Notebook Server with nb2kg Extension\n\n\nNow that you have the Jupyter Kernel Gateway Server running on z/OS, you now need to setup the Jupyter Notebook Server with nb2kg extension. There are a few ways to set this up, and choosing one depends on your environment. If you wish to run the Jupyter Notebook Server on a Linux on z LPAR, follow \nthese instructions.\n If you wish to run the Jupyter Notebook Server on an x86 Linux system, follow \nthese instructions.\n\n\nAuthors: David Rice, Jessie Yu    Date: April 5th, 2017",
            "title": "Jupyter with Remote Notebook on the Mainframe"
        },
        {
            "location": "/spark/jupyter-remote-notebook/#jupyter-kernel-gateway-on-the-mainframe",
            "text": "What is  Jupyter Kernel Gateway?  Jupyter Kernel Gateway has several capabilities, but one of these capabilities is to offer Jupyter Notebook kernels to remote hosts. The kernels will run on the mainframe but can be executed by an x86 system.   The diagram above shows how this process works. An end user would connect to a Jupyter Notebook Server with the nb2kg extension, in this case running as a  Docker  container. The nb2kg extension would connect to a Jupyter Kernel Gateway Server and would request access to the kernels it hosts. The Jupyter Kernel Gateway Server, which in this case runs on z/OS, would offer the kernels it hosts to the Jupyter Notebook Server that requested them, and then would run any notebooks on platform. In this reference architecture, one of the kernels the Jupyter Kernel Gateway Server would be offering is the Apache Toree kernel which offers the user an Apache Spark, Scala, Notebook environment to run under. Because the Apache Toree kernel is running on z/OS, there are no byte order issues.",
            "title": "Jupyter Kernel Gateway on the Mainframe"
        },
        {
            "location": "/spark/jupyter-remote-notebook/#setup-jupyter-notebook-to-kernel-gateway-w-apache-toree-environment",
            "text": "",
            "title": "Setup Jupyter Notebook to Kernel Gateway w/ Apache Toree Environment"
        },
        {
            "location": "/spark/jupyter-remote-notebook/#setup-the-kernel-gateway-and-apache-toree-on-zos",
            "text": "Given that this example revolves around the mainframe, a lot of the work will be done on a z/OS host. To install and run the Jupyter Kernel Gateway with Apache Toree solution on z/OS follow these  instructions.",
            "title": "Setup the Kernel Gateway and Apache Toree on z/OS"
        },
        {
            "location": "/spark/jupyter-remote-notebook/#setup-the-jupyter-notebook-server-with-nb2kg-extension",
            "text": "Now that you have the Jupyter Kernel Gateway Server running on z/OS, you now need to setup the Jupyter Notebook Server with nb2kg extension. There are a few ways to set this up, and choosing one depends on your environment. If you wish to run the Jupyter Notebook Server on a Linux on z LPAR, follow  these instructions.  If you wish to run the Jupyter Notebook Server on an x86 Linux system, follow  these instructions.  Authors: David Rice, Jessie Yu    Date: April 5th, 2017",
            "title": "Setup the Jupyter Notebook Server with nb2kg Extension"
        },
        {
            "location": "/spark/wlm-metering/",
            "text": "WLM Metering and Capping for IzODA Spark\n\n\n\nIntroduction\n\n\nOne of the strengths of the z Systems platform and the z/OS operating system is the option to run multiple workloads at the same time within one z/OS image or across a Parallel Sysplex. Workloads usually have different, often competing, performance and resource requirements that must be balanced to make the best use of an installation's resources, maintain the highest possible throughput, and achieve the best possible system responsiveness. The function that makes this possible is dynamic workload management, which is implemented in the workload management component of z/OS.\n\n\nWith z/OS workload management (WLM), you define performance goals and assign a business importance to each goal. You define the goals for work in business terms, and the system decides how much resource, such as CPU or memory, to assign so as to meet the goal. WLM constantly monitors the system and adapts processing to meet the goals.\n\n\nIn October 2016, IBM announced the intent to stage support for new capabilities in z/OS for metering and capping workloads over CPU and memory consumption. This support provides additional WLM controls that enable the system capacity planner to specify that the resource consumption of selected workloads should not exceed the specified limits. You can, for example, use WLM to ensure that all the applications or jobs submitted to a Spark cluster do not consume more than a specified real memory limit, and that they do not receive help from standard processors when the zIIP capacity is exceeded.\n\n\nThis paper provides an overview of using WLM Metering and Capping for IBM Open Data Analytics for z/OS Spark (IzODA Spark, FMID HSPK120), and its associated considerations.\n\n\nNote:\n The IBM.Function.MemoryCapping fix category and the MEMCAP/K keyword in RETAIN identify the PTFs that enable the Metering and Capping functions.\n\n\nOverview of Apache Spark processes\n\n\nIzODA Spark is built on Apache Spark, which uses a master/worker architecture. A Spark cluster typically has multiple processes, each running in its own Java virtual machine (JVM).\n\n\nThe following list describes the most significant processes:\n   \n\n       \nThe \nmaster\n daemon allocates resources across applications.\n\n       \nThe \nworker\n daemon monitors and reports resource availability and, when directed by the master, spawns executors. The worker also monitors the liveness and resource consumption of the executors.\n\n       \nThe \nexecutor\n performs the actual computation and data processing for the application.\n\n       \nThe \ndriver\n program runs the main function of the application and creates a \nSparkContext\n.\n\n  \n\n   The master and worker processes are generally lightweight. By contrast, the executors attempt to use all available system resources by default. You can use the spark-defaults.conf file, spark-env.sh file, and command line parameters to constrain the number of CPU cores and amount of memory that each executor can consume. These parameters, however, are static and require that the Spark processes be restarted for changes to take effect. (For more information about the Apache Spark configuration options, see \nSpark Configuration.\n For more information about tuning the Apache Spark cluster, see \nTuning Spark.\n\n\nWLM provides a more dynamic way to manage the performance of your Spark workloads.\n\n\nOverview of WLM classification for IzODA Spark\n\n\nYou specify goals for the WLM services for IzODA Spark work in the same manner as for other z/OS workloads, by associating the work with a service class. In the service class, you assign goals to the work, along with the relative importance of each goal. You can also assign limits for CPU and memory capacity to be available to a service class. To associate incoming work with a particular service class, you must also define classification rules.\n\n\nWLM uses classification rules to map work coming into the system to a specific service class and report class. The classification is based on work qualifiers. The first qualifier is the subsystem type that receives the work request. The subsystem type for work that is processed in z/OS UNIX (which IzODA Spark workloads are) is OMVS. You can then use the Spark process job names and/or user IDs as the secondary qualifiers to classify your Spark work. Based on your service (PTF) level, there are different ways to assign job names to your Spark processes.\n\n\nFor more information about configuring WLM for IzODA Spark, see \u201cConfiguring z/OS workload manager for Apache Spark\u201d in \nIBM Open Data Analytics for z/OS Installation and Customization Guide.\n\n\n\n   Figure 1. Example of WLM classification configuration\n\n\nFigure 1 shows a sample WLM configuration. In this example, 4 service classes are defined: ODAS1SCH, ODAS2SCH, ODAS2SCL, and ODAS2SCD.\n   \n\n       \nThe ODAS2SCH and ODAS2SCL service classes are associated with the ODAS2RGH resource group, which allows 50 percent General Processor (GP) capacity and has no memory limit.\n\n       \nThe ODAS2SCD service class is associated with the ODAS2RGL resource group, which allows only 10 percent GP capacity and has a memory limit of 20 GB.\n\n  \n\n   Four classification rules are defined that classify work as follows:\n   \n\n       \nAll processes in Spark cluster 1, whose names match the ODAS%1 pattern, are classified into the ODAS1SCH service class.\n\n       \nMaster and worker processes in Spark cluster 2, whose names match the ODAS%2* pattern, are classified into the ODAS2SCH service class.\n\n       \nThe executor for user Jill in Spark cluster 2 has a job name of ODASX002 and is classified into the ODAS2SCL service class.\n\n       \nThe executor for user Bob in Spark cluster 2 has a job name of ODASX003 and is classified into the ODAS2SCD service class.\n\n   \n\n\nConsiderations for using WLM Metering and Capping\n\n\nSpecifying Memory Limit for a resource group\n\n\nA WLM resource group is a way of limiting or guaranteeing the system resource availability to one or more service classes. WLM APAR OA52611 provides the ability to limit the physical memory, at a system level, consumed by all address spaces that belong to a resource group.\n\n\nExample:\n The following figure shows an example of creating a resource group called ODASRG with a maximum capacity of 10 percent of the LPAR share in the general processor pool and a memory limit of 20 GB:\n\n\n\n\nFigure 2. Example of creating a resource group\n\n\nIf the physical memory consumption by address spaces associated with a resource group is at or near its memory limit, the system may take action against the address spaces. For instance, the system might initiate paging (within the address spaces associated with the resource group), suspend storage requesters, or, in certain situations, abnormally end (ABEND) entire address spaces. It is therefore a good practice to use the memory limit attribute as an upper limit so that, under normal conditions, resource consumption will operate well below the limit.\n\n\nSpark executors are generally good candidates to have their physical memory usage capped, since they typically consume a lot of memory. However, you might see performance degradation or even termination of your Spark applications if they reach their memory limit. In the case of termination, Spark applications usually observe an ABEND SEC6. Another situation that may occur is if an executor is not executing or responding for an extended period of time, such as in a heavy paging situation initiated by approaching a resource group\u2019s memory limit, the executor may be declared lost by the driver. This may prompt the spawning of a new replacement executor, and it becomes possible to have two or more executors running in the same resource group performing the same computation. In a highly competitive resource situation, the driver may declare that it cannot finish successfully and terminates.\n\n\nTo ensure that the Spark cluster stays alive even if the system terminates executors, we recommend that you avoid placing the Spark master and worker daemons in a memory-limited resource group. We also recommend that you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_MEMORY parameter, for example, should be set to a value less than or equal to the memory limit defined for the executors' resource group. Setting appropriate values for these parameters helps Spark to dispatch work within its limits and help reduce unintended consequences. For more information about the Apache Spark configuration options for standalone mode, see \nSpark Standalone Mode.\n\n\nSpecifying Honor Priority for a service class\n\n\nWLM APAR OA52611 also provides the ability to exclude a service class from the system-wide honor priority defaults. By setting the honor priority attribute to NO on a service class, work in that service class that is eligible to run on specialty processors (such as zIIPs) does not overflow to general-purpose CPs (GPs) when there is insufficient capacity on the specialty processors. The default is to use the IFAHONORPRIORITY and IIPHONORPRIORITY parameters that are specified in the IEAOPTxx member of parmlib.\n   You can set the honor priority attribute to NO if you want to minimize the amount of Spark work processed by the GPs. However, give careful consideration to setting honor priority attributes to NO on service classes, especially in the case of highly utilized specialty processors. For instance, a Spark service class restricted to zIIPs may consume much of the zIIP capacity and cause other zIIP-eligible workloads, such as IBM DB2, to overflow to GPs. In addition, Spark applications, by default, fall into the default OMVS service class. If the default OMVS service class is restricted to specialty engines, other processes in the same service class, such as terminals or ssh sessions, might become unresponsive. Spark applications might also experience timeouts during driver and executor communication if they are waiting for CPU time for too long. Monitor these timeouts and adjust the timeout values in the Spark configuration accordingly.\n\n\nAs with memory limit, we recommend you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_CORES parameter, for example, should be set to a value less than or equal to the number of processors available to the Spark executors after WLM-imposed limits.\n\n\nSetting the honor priority attribute to NO might also change the goals for your workload. For instance, if a Spark service class has high velocity goals and is set to use only zIIPs, these two settings might interfere with each other under certain conditions and cause an undesired state for the Spark applications and their workload priorities.\n\n\nSee the \nWLM APAR OA52611 text\n for more information on the WLM Metering and Capping support.\n\n\nAuthors: Jessie Yu (jessieyu@us.ibm.com), Michael Gildein (megildei@us.ibm.com), Kevin Carr (kgcarr@us.ibm.com).    Date: November 27th, 2017",
            "title": "WLM Metering and Capping for IzODA Spark"
        },
        {
            "location": "/spark/wlm-metering/#introduction",
            "text": "One of the strengths of the z Systems platform and the z/OS operating system is the option to run multiple workloads at the same time within one z/OS image or across a Parallel Sysplex. Workloads usually have different, often competing, performance and resource requirements that must be balanced to make the best use of an installation's resources, maintain the highest possible throughput, and achieve the best possible system responsiveness. The function that makes this possible is dynamic workload management, which is implemented in the workload management component of z/OS.  With z/OS workload management (WLM), you define performance goals and assign a business importance to each goal. You define the goals for work in business terms, and the system decides how much resource, such as CPU or memory, to assign so as to meet the goal. WLM constantly monitors the system and adapts processing to meet the goals.  In October 2016, IBM announced the intent to stage support for new capabilities in z/OS for metering and capping workloads over CPU and memory consumption. This support provides additional WLM controls that enable the system capacity planner to specify that the resource consumption of selected workloads should not exceed the specified limits. You can, for example, use WLM to ensure that all the applications or jobs submitted to a Spark cluster do not consume more than a specified real memory limit, and that they do not receive help from standard processors when the zIIP capacity is exceeded.  This paper provides an overview of using WLM Metering and Capping for IBM Open Data Analytics for z/OS Spark (IzODA Spark, FMID HSPK120), and its associated considerations.  Note:  The IBM.Function.MemoryCapping fix category and the MEMCAP/K keyword in RETAIN identify the PTFs that enable the Metering and Capping functions.",
            "title": "Introduction"
        },
        {
            "location": "/spark/wlm-metering/#overview-of-apache-spark-processes",
            "text": "IzODA Spark is built on Apache Spark, which uses a master/worker architecture. A Spark cluster typically has multiple processes, each running in its own Java virtual machine (JVM).  The following list describes the most significant processes:\n    \n        The  master  daemon allocates resources across applications. \n        The  worker  daemon monitors and reports resource availability and, when directed by the master, spawns executors. The worker also monitors the liveness and resource consumption of the executors. \n        The  executor  performs the actual computation and data processing for the application. \n        The  driver  program runs the main function of the application and creates a  SparkContext . \n   \n   The master and worker processes are generally lightweight. By contrast, the executors attempt to use all available system resources by default. You can use the spark-defaults.conf file, spark-env.sh file, and command line parameters to constrain the number of CPU cores and amount of memory that each executor can consume. These parameters, however, are static and require that the Spark processes be restarted for changes to take effect. (For more information about the Apache Spark configuration options, see  Spark Configuration.  For more information about tuning the Apache Spark cluster, see  Tuning Spark.  WLM provides a more dynamic way to manage the performance of your Spark workloads.",
            "title": "Overview of Apache Spark processes"
        },
        {
            "location": "/spark/wlm-metering/#overview-of-wlm-classification-for-izoda-spark",
            "text": "You specify goals for the WLM services for IzODA Spark work in the same manner as for other z/OS workloads, by associating the work with a service class. In the service class, you assign goals to the work, along with the relative importance of each goal. You can also assign limits for CPU and memory capacity to be available to a service class. To associate incoming work with a particular service class, you must also define classification rules.  WLM uses classification rules to map work coming into the system to a specific service class and report class. The classification is based on work qualifiers. The first qualifier is the subsystem type that receives the work request. The subsystem type for work that is processed in z/OS UNIX (which IzODA Spark workloads are) is OMVS. You can then use the Spark process job names and/or user IDs as the secondary qualifiers to classify your Spark work. Based on your service (PTF) level, there are different ways to assign job names to your Spark processes.  For more information about configuring WLM for IzODA Spark, see \u201cConfiguring z/OS workload manager for Apache Spark\u201d in  IBM Open Data Analytics for z/OS Installation and Customization Guide.  \n   Figure 1. Example of WLM classification configuration  Figure 1 shows a sample WLM configuration. In this example, 4 service classes are defined: ODAS1SCH, ODAS2SCH, ODAS2SCL, and ODAS2SCD.\n    \n        The ODAS2SCH and ODAS2SCL service classes are associated with the ODAS2RGH resource group, which allows 50 percent General Processor (GP) capacity and has no memory limit. \n        The ODAS2SCD service class is associated with the ODAS2RGL resource group, which allows only 10 percent GP capacity and has a memory limit of 20 GB. \n   \n   Four classification rules are defined that classify work as follows:\n    \n        All processes in Spark cluster 1, whose names match the ODAS%1 pattern, are classified into the ODAS1SCH service class. \n        Master and worker processes in Spark cluster 2, whose names match the ODAS%2* pattern, are classified into the ODAS2SCH service class. \n        The executor for user Jill in Spark cluster 2 has a job name of ODASX002 and is classified into the ODAS2SCL service class. \n        The executor for user Bob in Spark cluster 2 has a job name of ODASX003 and is classified into the ODAS2SCD service class.",
            "title": "Overview of WLM classification for IzODA Spark"
        },
        {
            "location": "/spark/wlm-metering/#considerations-for-using-wlm-metering-and-capping",
            "text": "",
            "title": "Considerations for using WLM Metering and Capping"
        },
        {
            "location": "/spark/wlm-metering/#specifying-memory-limit-for-a-resource-group",
            "text": "A WLM resource group is a way of limiting or guaranteeing the system resource availability to one or more service classes. WLM APAR OA52611 provides the ability to limit the physical memory, at a system level, consumed by all address spaces that belong to a resource group.  Example:  The following figure shows an example of creating a resource group called ODASRG with a maximum capacity of 10 percent of the LPAR share in the general processor pool and a memory limit of 20 GB:   Figure 2. Example of creating a resource group  If the physical memory consumption by address spaces associated with a resource group is at or near its memory limit, the system may take action against the address spaces. For instance, the system might initiate paging (within the address spaces associated with the resource group), suspend storage requesters, or, in certain situations, abnormally end (ABEND) entire address spaces. It is therefore a good practice to use the memory limit attribute as an upper limit so that, under normal conditions, resource consumption will operate well below the limit.  Spark executors are generally good candidates to have their physical memory usage capped, since they typically consume a lot of memory. However, you might see performance degradation or even termination of your Spark applications if they reach their memory limit. In the case of termination, Spark applications usually observe an ABEND SEC6. Another situation that may occur is if an executor is not executing or responding for an extended period of time, such as in a heavy paging situation initiated by approaching a resource group\u2019s memory limit, the executor may be declared lost by the driver. This may prompt the spawning of a new replacement executor, and it becomes possible to have two or more executors running in the same resource group performing the same computation. In a highly competitive resource situation, the driver may declare that it cannot finish successfully and terminates.  To ensure that the Spark cluster stays alive even if the system terminates executors, we recommend that you avoid placing the Spark master and worker daemons in a memory-limited resource group. We also recommend that you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_MEMORY parameter, for example, should be set to a value less than or equal to the memory limit defined for the executors' resource group. Setting appropriate values for these parameters helps Spark to dispatch work within its limits and help reduce unintended consequences. For more information about the Apache Spark configuration options for standalone mode, see  Spark Standalone Mode.",
            "title": "Specifying Memory Limit for a resource group"
        },
        {
            "location": "/spark/wlm-metering/#specifying-honor-priority-for-a-service-class",
            "text": "WLM APAR OA52611 also provides the ability to exclude a service class from the system-wide honor priority defaults. By setting the honor priority attribute to NO on a service class, work in that service class that is eligible to run on specialty processors (such as zIIPs) does not overflow to general-purpose CPs (GPs) when there is insufficient capacity on the specialty processors. The default is to use the IFAHONORPRIORITY and IIPHONORPRIORITY parameters that are specified in the IEAOPTxx member of parmlib.\n   You can set the honor priority attribute to NO if you want to minimize the amount of Spark work processed by the GPs. However, give careful consideration to setting honor priority attributes to NO on service classes, especially in the case of highly utilized specialty processors. For instance, a Spark service class restricted to zIIPs may consume much of the zIIP capacity and cause other zIIP-eligible workloads, such as IBM DB2, to overflow to GPs. In addition, Spark applications, by default, fall into the default OMVS service class. If the default OMVS service class is restricted to specialty engines, other processes in the same service class, such as terminals or ssh sessions, might become unresponsive. Spark applications might also experience timeouts during driver and executor communication if they are waiting for CPU time for too long. Monitor these timeouts and adjust the timeout values in the Spark configuration accordingly.  As with memory limit, we recommend you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_CORES parameter, for example, should be set to a value less than or equal to the number of processors available to the Spark executors after WLM-imposed limits.  Setting the honor priority attribute to NO might also change the goals for your workload. For instance, if a Spark service class has high velocity goals and is set to use only zIIPs, these two settings might interfere with each other under certain conditions and cause an undesired state for the Spark applications and their workload priorities.  See the  WLM APAR OA52611 text  for more information on the WLM Metering and Capping support.  Authors: Jessie Yu (jessieyu@us.ibm.com), Michael Gildein (megildei@us.ibm.com), Kevin Carr (kgcarr@us.ibm.com).    Date: November 27th, 2017",
            "title": "Specifying Honor Priority for a service class"
        },
        {
            "location": "/odl/odl/",
            "text": "Optimized Data Layer\n\n\n\nThe Optimized Data Layer is also known as the \nIBM Mainframe Data Service for Apache Spark\n (MDS, or MDSS). It is a started task in the z/OS address space that provides a common interface to several different mainframe and off-platform data sources. The kinds of data sources that can be accessed through ODL include:\n\n\n    \nIBM DB2\n\n    \nIBM Virtual Storage Access Method (VSAM)\n\n    \nIBM Information Management System (IMS)\n\n    \nPartitioned Data Sets (PDS)\n\n    \nz/OS sequential files (including log files)\n\n    \nADABAS\n\n    \nData from other platforms:\n      \n\n        \nDatabases - SQL and NoSQL (e.g. MongoDB, Oracle, Teradata)\n\n        \nStreaming feeds (e.g. Twitter, Facebook)\n\n      \n\n    \n\n\n\nAccess to these data sources is provided by:\n\n\n    \nJDBC for Java and Scala applications - usually run from the Spark environment.\n\n    \ndsdbc for Python applications running in the Anaconda environment\n\n\n\nAlthough ODL was originally developed to provide a JDBC interface to Scala and Java applications that use the Spark stack, it has been extended for IzODA to service Python applications through the dsdbc interface. Examples of both types of data access can be found in our\n\n\nODL includes a workstation based interface called the Data Service Studio (DSS) that is used to create, view, and change metadata describing the mapping from mainframe data sources to a relational view of the data on provided by ODL on z/OS. DSS is based on Eclipse. It can either be installed separately, or can be added to an existing Eclipse shell.\n\n\nFor more details about the z/OS and workstation-based components of ODL and how they relate to one another, as well as details about alternative methods for virtualizing database views, please see:\n\n\n    \nChapter 2 of the \nIzODA Administrator's Guide\n\n    \nChapter 3 of the \nApache Spark Redbook\n\n\n\n\nInstallation and Customization\n\n\nInstructions about how to install and customize the various components of ODL are in the chapters of \nIzODA Installation and Customization Guide\n that reference \nMDS\n and the \nData Service Studio.\n\n\nVerify Your Installation\n\n\nODL is intended to be used together with one or both of the analytics stacks of IzODA. We have created a collection of small sample applications which you can run to verify that ODL and the IzODA analytics stack of your choice is installed and configured properly. Please choose one of the Install Verification Programs (IVPs) from our docs to ensure your configuration is correct.\n\n\nReference Material\n\n\nA library of documentation is available for the \nIBM Open Data Analytics for z/OS\n offering. This can be found at the \nIBM Knowledge Center.\n\n\nThe \nApache Spark Implementation on IBM z/OS\n is another good source of information about both Spark and ODL.",
            "title": "Overview"
        },
        {
            "location": "/odl/odl/#installation-and-customization",
            "text": "Instructions about how to install and customize the various components of ODL are in the chapters of  IzODA Installation and Customization Guide  that reference  MDS  and the  Data Service Studio.",
            "title": "Installation and Customization"
        },
        {
            "location": "/odl/odl/#verify-your-installation",
            "text": "ODL is intended to be used together with one or both of the analytics stacks of IzODA. We have created a collection of small sample applications which you can run to verify that ODL and the IzODA analytics stack of your choice is installed and configured properly. Please choose one of the Install Verification Programs (IVPs) from our docs to ensure your configuration is correct.",
            "title": "Verify Your Installation"
        },
        {
            "location": "/odl/odl/#reference-material",
            "text": "A library of documentation is available for the  IBM Open Data Analytics for z/OS  offering. This can be found at the  IBM Knowledge Center.  The  Apache Spark Implementation on IBM z/OS  is another good source of information about both Spark and ODL.",
            "title": "Reference Material"
        },
        {
            "location": "/odl/credit-risk-odl/",
            "text": "Credit Risk Demonstration\n\n\n\nPlease see the \nCredit Risk Example on our Github\n for further information.",
            "title": "Credit Risk Assessment Example"
        },
        {
            "location": "/anaconda/anaconda/",
            "text": "Anaconda\n\n\n\nAnaconda\n is an assembly of parts that implement a Python-based analytics stack. It consists of multiple components:\n\n\n   \nPython 3.6.1\n\n   \nThe conda package manager\n\n   \nA library of locally installed open source packages\n\n   \nA \nchannel, or reference repository\n located on the \nAnaconda Cloud\n\n\n\n\nReference Architecture\n\n\nThe environment above shows a user interacting with the Anaconda/Python stack using the Jupyter notebook ecosystem. Users may also perform analytics by running python applications directly from the command line of a shell session as they would any other python script.\n\n\nInstallation and Customization\n\n\nPlease refer to our \nAnaconda installation and configuration page.\n\n\nVerify Your Installation\n\n\nWe have created a collection of small sample applications which you can run to verify that Anaconda is installed and configured properly. Please choose one of the following \nInstall Verification Programs (IVPs)\n below to ensure your configuration is correct.\n\n\n   \nIVP with ODL and Jupyter Notebook:\n This IVP demonstrates the Anaconda stack through a Jupyter Notebook and retrieves the data from ODL using a python module.\n\n   \nIVP with Pyspark:\n This IVP demonstrates the Spark stack through a Python Jupyter notebook that uses Pyspark (Python API to Spark) to illustrate the use of Spark dataframes.\n\n\n\n\nRestrictions and Usage Notes\n\n\nThere are some known issues with the Anaconda environment of IzODA. To avoid these issues, here is a list of restrictions on the functionality of Anaconda:\n\n\n\n\n\n\n\n\nRestrictions\n\n\nMitigations\n\n\n\n\n\n\n\n\n\n\nThe interactive backend packages for Matplotlib is not currently supported. This includes, GTK, WX, Cairo, etc. The user has the option instead, to use a non-interactive backend, capable of writing to a file.\n\n\nUse  Matplotlib with the Seaborn backend, or use Bokeh for graphical visualization.\n\n\n\n\n\n\nThe conda and python package levels must be kept in sync. Arbitrary combinations of conda and python package levels may result in errors.\n\n\nThe supported conda and python version combinations are: \nconda (version 4.3.14, build py36_4) and python (version 3.6.1, build 13)\nconda (version 4.3.17, build py36_6) and python (version 3.6.1, build 24)\n\n\n\n\n\n\nDistributed Dask capabilities are not yet supported. \u201cBig Data\u201d Dask collections can be used, and distributed capabilities will be made in beta form through the IzODA channel.\n\n\n\n\n\n\n\n\nCannot write to HDF5 files using Dask\u2019s APIs.\n\n\n\n\n\n\n\n\nWhen creating an \"empty\" conda environment, be sure to include the Python package. Failing to do so can result in Python being unable to find its main shared library (libpython3.6m.so). If you have an environment active and install a new package into it, please exit and re-enter the environment using source deactivate/activate to ensure that all path variables in the environment are set properly.\n\n\n\n\n\n\n\n\n\n\nUsage Notes\n\n\nSeveral additional capabilities ship with Anaconda that have not yet been fully verified. These are not restrictions, but we advise that these are in beta form, and are not yet fully supported:\n\n\n   \nPackage installation from platform agnostic channels at anaconda.org\n\n   \nPackage installation using the Python Packaging Authority Installer (PIP)\n\n   \nJupyter notebook server on z/OS. We recommend continued use of our current Kernel Gateway/Apache Toree offering in concert with a Linux-based Jupyter notebook server.\n\n   \nAnaconda users should run bash version 4.3.48 or later. \nNote: Apache Spark requires version 4.2.53.\n Recommended use is to make this the user's top level shell through the PROGRAM setting in the OMVS segment of their RACF profile. Using conda from arbitrarily nested shell sessions is not recommended.\n\n   \nSome operations with the Dask package may result in error messages about broken pipes\n\n\n\nWork is continuing to resolve the problems underlying the restriction set. We will remove restrictions as they are resolved, and fully support the beta functions above as verification is completed.",
            "title": "Overview"
        },
        {
            "location": "/anaconda/anaconda/#reference-architecture",
            "text": "The environment above shows a user interacting with the Anaconda/Python stack using the Jupyter notebook ecosystem. Users may also perform analytics by running python applications directly from the command line of a shell session as they would any other python script.",
            "title": "Reference Architecture"
        },
        {
            "location": "/anaconda/anaconda/#installation-and-customization",
            "text": "Please refer to our  Anaconda installation and configuration page.",
            "title": "Installation and Customization"
        },
        {
            "location": "/anaconda/anaconda/#verify-your-installation",
            "text": "We have created a collection of small sample applications which you can run to verify that Anaconda is installed and configured properly. Please choose one of the following  Install Verification Programs (IVPs)  below to ensure your configuration is correct. \n    IVP with ODL and Jupyter Notebook:  This IVP demonstrates the Anaconda stack through a Jupyter Notebook and retrieves the data from ODL using a python module. \n    IVP with Pyspark:  This IVP demonstrates the Spark stack through a Python Jupyter notebook that uses Pyspark (Python API to Spark) to illustrate the use of Spark dataframes.",
            "title": "Verify Your Installation"
        },
        {
            "location": "/anaconda/anaconda/#restrictions-and-usage-notes",
            "text": "There are some known issues with the Anaconda environment of IzODA. To avoid these issues, here is a list of restrictions on the functionality of Anaconda:     Restrictions  Mitigations      The interactive backend packages for Matplotlib is not currently supported. This includes, GTK, WX, Cairo, etc. The user has the option instead, to use a non-interactive backend, capable of writing to a file.  Use  Matplotlib with the Seaborn backend, or use Bokeh for graphical visualization.    The conda and python package levels must be kept in sync. Arbitrary combinations of conda and python package levels may result in errors.  The supported conda and python version combinations are:  conda (version 4.3.14, build py36_4) and python (version 3.6.1, build 13) conda (version 4.3.17, build py36_6) and python (version 3.6.1, build 24)    Distributed Dask capabilities are not yet supported. \u201cBig Data\u201d Dask collections can be used, and distributed capabilities will be made in beta form through the IzODA channel.     Cannot write to HDF5 files using Dask\u2019s APIs.     When creating an \"empty\" conda environment, be sure to include the Python package. Failing to do so can result in Python being unable to find its main shared library (libpython3.6m.so). If you have an environment active and install a new package into it, please exit and re-enter the environment using source deactivate/activate to ensure that all path variables in the environment are set properly.",
            "title": "Restrictions and Usage Notes"
        },
        {
            "location": "/anaconda/anaconda/#usage-notes",
            "text": "Several additional capabilities ship with Anaconda that have not yet been fully verified. These are not restrictions, but we advise that these are in beta form, and are not yet fully supported: \n    Package installation from platform agnostic channels at anaconda.org \n    Package installation using the Python Packaging Authority Installer (PIP) \n    Jupyter notebook server on z/OS. We recommend continued use of our current Kernel Gateway/Apache Toree offering in concert with a Linux-based Jupyter notebook server. \n    Anaconda users should run bash version 4.3.48 or later.  Note: Apache Spark requires version 4.2.53.  Recommended use is to make this the user's top level shell through the PROGRAM setting in the OMVS segment of their RACF profile. Using conda from arbitrarily nested shell sessions is not recommended. \n    Some operations with the Dask package may result in error messages about broken pipes  \nWork is continuing to resolve the problems underlying the restriction set. We will remove restrictions as they are resolved, and fully support the beta functions above as verification is completed.",
            "title": "Usage Notes"
        },
        {
            "location": "/anaconda/packages/",
            "text": "IzODA Anaconda Packages\n\n\n\nThe following are the packages provided by the \nIzODA Anaconda Channel.\n\n\n\n\n\n  \n\n    \n\n      \n\n        \n\n          \nLabel/Release Level: \n\n          \n\n            \nmain\n\n          \n\n        \n\n        \n\n          \n\n        \n\n        \n\n          \nGenerate Environment Script\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n  \n\n    \nPackage\n\n    \nVersion\n\n    \nLicense\n\n    \nDescription",
            "title": "Packages"
        },
        {
            "location": "/anaconda/install-config/",
            "text": "Anaconda Installation and Configuration\n\n\n\nThis process installs all of the components of a fully a functional Anaconda deployment. There are some key characteristics of the z/OS environment to be aware of:\n\n\n   \n  \nAll Anaconda components are 64-bit ASCII applications.\n\n  \nSince Anaconda is an ASCII environment running on a platform with a default EBCDIC code page, file tagging is used extensively. Be aware that newly created files in this environment must be properly tagged for everything to run smoothly.\n  \n\n  \nThese instructions are for a system administrator running in a z/OS shell. They are tailored for a bash shell instance. If you are using the default \ntcsh\n that comes with z/OS, substitute the \nsetenv\n command for \nexport\n.\n\n  \nProper WLM classification of Python workloads depends on how the tasks and processes associated with the application are named. By default, all such tasks will be associated with the name of the invoking user. However, this gives no indication to WLM that the given task is part of a Python workload. See the environmental setup section below for an explanation about how to associate a job name with the tasks of your Python application that WLM can recognize.\n\n\n\n\n\nPrerequisites\n\n\n\n   \nIzODA installs Anaconda from a local archive shipped as part of the SMP/E offering. Because of this, the install process requires room for the archive, plus about the same amount of space in the target file system that contains the Anaconda home directory - usually \n/usr/lpp/IBM/izoda/anaconda\n. Since Anaconda ships with approximately 230 packages, a finished install consumes between 2.5 and 3 GB.\n\n\nIn addition to the disk space required during the IzODA install, it's important to remember that Anaconda keeps all package versions that have been installed from the IzODA channel through the conda command. This allows previous versions of an installed package to be referenced in addition to the latest version. Be sure to give Anaconda enough room to operate. The minimum disk space for an Anaconda installation is 6 GB, and the recommended amount is 20 GB.\n\n   \nPython makes use of the \n/dev/urandom\n character special file to read pseudo-random values for use in a variety of applications. The \nIntegrated Cryptographic Service Facility (ICSF)\n service of z/OS must be started for \n/dev/urandom\n to function properly. Failures will occur if ICSF is not active when Anaconda is installed.\n\n\n head -c40 /dev/urandom\n head: /dev/urandom: EDC5157I An internal error has occurred.\n\n\n\nThe pre-requisite level of bash is 4.2.53. This is the version installed as the default during Anaconda installation. A 4.3.46 version of bash is also included as an Anaconda package that can be installed via conda. This is a functional version of the shell, but this is not officially supported with either Spark or Anaconda.\n\n\n\n\nEnvironmental Setup\n\n\nThis is the setup necessary for any user to run Python applications and make use of other Anaconda functions:\n\n\n   \nInclude this setup in your .bashrc profile to make file conversion and tagging transparent to your Python application:\n\n\n export _CEE_RUNOPTS=\"FILETAG(AUTOCVT,AUTOTAG) POSIX(ON)\"\n if [[ \"x$_BPXK_AUTOCVT\" == \"x\" ]]; then\n   export _BPXK_AUTOCVT=ON\n   exec $BASH \"$@\"\n fi\n\n\n\nAdd the Anaconda bin directory to your PATH in .bashrc:\n\n\nexport ANACONDA_ROOT=\"/usr/lpp/IBM/izoda/anaconda\"\n\n\nexport PATH=$ANACONDA_ROOT/bin:$PATH\n\n\nThe Anaconda bin directory doesn't have to be first in your PATH, but if there are other shells or versions of bash installed on the system, the Anaconda bin directory has to preceed them in the PATH. Note: substitute the actual path to the Anaconda root from your environment when exporting ANACONDA_ROOT.\n\n   \nMost Python applications will use the Optimized Data Layer to access data sources on z/OS and from remote servers. These call to ODL through the dsdbc api. Set up your STEPLIB to include the load library for this interface in .bashrc:\n\n\n export STEPLIB=hlq.SAZKLOAD:$STEPLIB\n\n\n\nwhere hlq is the high-level qualifier of the ODL (MDS) installation.\n\n   \nGrant users running Python applications READ permission to the BPX.JOBNAME facility class profile so that they can assign jobnames to their workload. Please see the \n_BPX environment variables section\n of the z/OS UNIX System Services Planning guide for more information. Note: superusers are allowed to set jobnames regardless of their access to BPX.JOBNAME. \nIn general, we recommend against Python users having superuser privileges.\n\n\n\n\nDownload and Unpack\n\n\nIzODA installs using SMP/E. Instructions are in the \nProgram Directory.\n The PSP bucket for Anaconda 1.1.0 should be reviewed prior to installation; see also the \nLatebreaking News\n section of this page.\n\n\nInstall\n\n\nThe SMP/E jobs need to be run in order. Each job includes its own instructions at the top.\n\n\nPost-SMP/E Installation Instructions\n\n\nThere are several additional steps that a system administrator may wish to perform, depending on the runtime configuration they intend to create. Before running any of the following steps, cd to the root directory of the of the Anaconda installation. Note that all of the tools below are in the bin directory under the Anaconda root directory.\n\n\n   \nA common method of installing service involves mounting the filesystem containing Anaconda at a different location from normal, and in read_write mode. After installing service, the filesystem is re-mounted at is usual location, and may be in read-only mode. If you use this method, you must run change-prefix to fix the pathnames that are imbedded in many of the scripts. change-prefix takes two arguments, old-prefix and new-prefix. For example:\n\n\n change-prefix /service/usr/lpp/IBM/izoda/anaconda /usr/lpp/IBM/izoda/anaconda\n\n\n\nPlease note that change-prefix is not included in the GA release of IzODA, but is included in the first PTF. You can download change-prefix here, then upload it to your system (in binary mode), then cd to the Anaconda root directory and unpack it with \"pax -r -f change-prefix.pax\".\n\n   \nBash 4.2 requires shell scripts to be encoded in EBCDIC, even though all Python scripts in Anaconda are ASCII, and are file tagged to indicate this. The \ninstall_ensure_scripts_are_in_ebcdic\n tool can be used to guarantee all shell scripts in the Anaconda installation have the proper encoding.\n\n   \nA good practice is to install Anaconda read-only for all users except an owning administrator. You can use the \ninstall_set_single_anaconda_admin\n tool to set the group and other permissions for All Anaconda parts to read-only.\n   \nIf you wish to set up Anaconda to be managed by a group of administrators, use the \ninstall_set_shared_anaconda_admin\n tool. Run the command like this:\n\n\n install_set_shared_anaconda_admin groupname\n\n\n\nwhere groupname is the name of the administrators group.\n\n   \nIf you plan to run Python code in a server (as a daemon), you should run the \ninstall_set_program_control\n tool. For more information, refer to the \nDefining programs in UNIX files to program control\n section of the \nz/OS Unix System Services Planning guide.\n\n\n\nThere is one mandatory last step - run the \nconda\n command to pull all updates to the Anaconda environment from the IzODA channel on the Anaconda cloud.\n\n\nconda update\n\n\nconda install bash=4.2\n\n\nThis step is necessary because updates are frequently made to the packages of the Anaconda root environment. The PID version of IzODA will usually be downlevel from the version that exists in the IzODA Anaconda channel. If you use Spark, you should also run the \nconda install\n to restore the version of bash that has been tested with Spark.\n\n\nActivate an Environment\n\n\nA successful Anaconda installation results in a root environment that includes all of the packages shipped with IzODA. The root environment can be activated with the Anaconda \nactivate\n command.\n\n\nNote: as a temporary restriction, anaconda commands must be run from a bash sub-shell. If you have established a login with one bash session active, run bash again from the command line to create the necessary shell nesting before using Anaconda commands.\n\n\nIn order to affect the current shell environment, the Anaconda activate command must be run using the \nsource\n bash builtin:\n\n\n/home/condausr> bash\n\n\n/home/condausr> source activate root\n\n\n(root) /home/condausr>\n\n\nNote that the name of the active Anaconda environment is in parentheses before any command line prompt that you have have set up. At this point, any packages in the active environment are available to applications that you may want to run.\n\n\nIn order to exit the root environment and return to you original shell, use the Anaconda \ndeactivate\n command:\n\n\n(root) /home/condausr> source deactivate root\n\n\n/home/condausr>\n\n\nA complete description of conda and its functions is available in the \nconda User Guide\n.\n\n\nLatebreaking News\n\n\nThis section augments information in the \nPSP bucket for Anaconda 1.1.0\n.\n\n\nApplying maintenance\n\n\nAnaconda takes a new approach to software maintenance. Rather than require all users on a system to use the same version of a given tool, Anaconda allows each user to select whatever version they wish to run, by controlling the content of their \nconda environment\n. Users can create as many environments as they wish, potentially using different versions of tools for different task.\n\n\nThis means that installing new versions of the tools and packages controlled by conda \nadds the new versions to the system without removing old versions\n. This allows users to transition to new versions on their own schedule.\n\n\nThat said, the \ndefault\n environment (also known as the \nroot\n environment) is managed by the system administrator, and usually provides the most recent versions of tools.\n\n\nThe preferred means of installing new code into the conda root environment is by using the \nconda update\n command. This will obtain the latest versions of packages and tools from a \nchannel\n. IBM provides the \nIzODA channel on the Anaconda cloud\n, which will be updated regularly with new packages, and new versions of existing packages. This channel is enabled by default when Anaconda is first installed.\n\n\nHowever, not all installations will choose to install maintenance via the Internet. For such installations, IBM will provide regular PTFs for use with SMP/E.\n\n\nNote, however, that installing maintenance via PTF will disable default access to the IzODA channel, under the presumption that subsequent maintenance will also be applied via SMP/E. Access can be restored by \naltering the conda configuration file.\n\n\nMaintaining the correct file ownership\n\n\nIf Anaconda is installed and maintained by a userid other than root, installation of maintenance will generally result in files in the Anaconda root directory having the owner and group IDs of that userid.\n\n\nIf, on the other hand, Anaconda is installed and maintained by root, files created or modified by installing maintenance (either via conda update or SMP/E) may produce files whose user and owner IDs are determined by the system that created the package. This is not desirable. To avoid this situation, execute the following commands, as root after installing maintenance:\n\n\ncd /usr/lpp/IBM/izoda/anaconda   # or the Anaconda root directory, if changed\n chown -R 0:0 .\n\n\nThis will force the uid/gid for all of the files in the entire Anaconda root directory (and subdirectories) to 0, which is reserved for root.\n\n\nTerminology\n\n\nroot\n\n\nThe term \"root\" has one of three meanings, in the context of Anaconda:\n\n\n   \nroot can mean the \nroot userid\n. This user has uid 0. However, in z/OS, there are other ways to get all privileges, and the use of uid of 0 is less common.\n\n   \nroot can mean the \nAnaconda root directory\n. This is usually /usr/lpp/IBM/izoda/anaconda. It can be changed at installation time, and will often be temporarily different while service is being applied.\n\n   \nroot can mean the \nroot conda environment\n. When you run the script named activate to activate a conda environment, it sets an environment variable that causes bash to print the environment name in parenthesis. The default environment is named root.",
            "title": "Installation and Configuration"
        },
        {
            "location": "/anaconda/install-config/#prerequisites",
            "text": "IzODA installs Anaconda from a local archive shipped as part of the SMP/E offering. Because of this, the install process requires room for the archive, plus about the same amount of space in the target file system that contains the Anaconda home directory - usually  /usr/lpp/IBM/izoda/anaconda . Since Anaconda ships with approximately 230 packages, a finished install consumes between 2.5 and 3 GB.  In addition to the disk space required during the IzODA install, it's important to remember that Anaconda keeps all package versions that have been installed from the IzODA channel through the conda command. This allows previous versions of an installed package to be referenced in addition to the latest version. Be sure to give Anaconda enough room to operate. The minimum disk space for an Anaconda installation is 6 GB, and the recommended amount is 20 GB. \n    Python makes use of the  /dev/urandom  character special file to read pseudo-random values for use in a variety of applications. The  Integrated Cryptographic Service Facility (ICSF)  service of z/OS must be started for  /dev/urandom  to function properly. Failures will occur if ICSF is not active when Anaconda is installed.   head -c40 /dev/urandom\n head: /dev/urandom: EDC5157I An internal error has occurred.  The pre-requisite level of bash is 4.2.53. This is the version installed as the default during Anaconda installation. A 4.3.46 version of bash is also included as an Anaconda package that can be installed via conda. This is a functional version of the shell, but this is not officially supported with either Spark or Anaconda.",
            "title": "Prerequisites"
        },
        {
            "location": "/anaconda/install-config/#environmental-setup",
            "text": "This is the setup necessary for any user to run Python applications and make use of other Anaconda functions: \n    Include this setup in your .bashrc profile to make file conversion and tagging transparent to your Python application:   export _CEE_RUNOPTS=\"FILETAG(AUTOCVT,AUTOTAG) POSIX(ON)\"\n if [[ \"x$_BPXK_AUTOCVT\" == \"x\" ]]; then\n   export _BPXK_AUTOCVT=ON\n   exec $BASH \"$@\"\n fi  Add the Anaconda bin directory to your PATH in .bashrc:  export ANACONDA_ROOT=\"/usr/lpp/IBM/izoda/anaconda\"  export PATH=$ANACONDA_ROOT/bin:$PATH  The Anaconda bin directory doesn't have to be first in your PATH, but if there are other shells or versions of bash installed on the system, the Anaconda bin directory has to preceed them in the PATH. Note: substitute the actual path to the Anaconda root from your environment when exporting ANACONDA_ROOT. \n    Most Python applications will use the Optimized Data Layer to access data sources on z/OS and from remote servers. These call to ODL through the dsdbc api. Set up your STEPLIB to include the load library for this interface in .bashrc:   export STEPLIB=hlq.SAZKLOAD:$STEPLIB  where hlq is the high-level qualifier of the ODL (MDS) installation. \n    Grant users running Python applications READ permission to the BPX.JOBNAME facility class profile so that they can assign jobnames to their workload. Please see the  _BPX environment variables section  of the z/OS UNIX System Services Planning guide for more information. Note: superusers are allowed to set jobnames regardless of their access to BPX.JOBNAME.  In general, we recommend against Python users having superuser privileges.",
            "title": "Environmental Setup"
        },
        {
            "location": "/anaconda/install-config/#download-and-unpack",
            "text": "IzODA installs using SMP/E. Instructions are in the  Program Directory.  The PSP bucket for Anaconda 1.1.0 should be reviewed prior to installation; see also the  Latebreaking News  section of this page.",
            "title": "Download and Unpack"
        },
        {
            "location": "/anaconda/install-config/#install",
            "text": "The SMP/E jobs need to be run in order. Each job includes its own instructions at the top.",
            "title": "Install"
        },
        {
            "location": "/anaconda/install-config/#post-smpe-installation-instructions",
            "text": "There are several additional steps that a system administrator may wish to perform, depending on the runtime configuration they intend to create. Before running any of the following steps, cd to the root directory of the of the Anaconda installation. Note that all of the tools below are in the bin directory under the Anaconda root directory. \n    A common method of installing service involves mounting the filesystem containing Anaconda at a different location from normal, and in read_write mode. After installing service, the filesystem is re-mounted at is usual location, and may be in read-only mode. If you use this method, you must run change-prefix to fix the pathnames that are imbedded in many of the scripts. change-prefix takes two arguments, old-prefix and new-prefix. For example:   change-prefix /service/usr/lpp/IBM/izoda/anaconda /usr/lpp/IBM/izoda/anaconda  Please note that change-prefix is not included in the GA release of IzODA, but is included in the first PTF. You can download change-prefix here, then upload it to your system (in binary mode), then cd to the Anaconda root directory and unpack it with \"pax -r -f change-prefix.pax\". \n    Bash 4.2 requires shell scripts to be encoded in EBCDIC, even though all Python scripts in Anaconda are ASCII, and are file tagged to indicate this. The  install_ensure_scripts_are_in_ebcdic  tool can be used to guarantee all shell scripts in the Anaconda installation have the proper encoding. \n    A good practice is to install Anaconda read-only for all users except an owning administrator. You can use the  install_set_single_anaconda_admin  tool to set the group and other permissions for All Anaconda parts to read-only.\n    If you wish to set up Anaconda to be managed by a group of administrators, use the  install_set_shared_anaconda_admin  tool. Run the command like this:   install_set_shared_anaconda_admin groupname  where groupname is the name of the administrators group. \n    If you plan to run Python code in a server (as a daemon), you should run the  install_set_program_control  tool. For more information, refer to the  Defining programs in UNIX files to program control  section of the  z/OS Unix System Services Planning guide.  \nThere is one mandatory last step - run the  conda  command to pull all updates to the Anaconda environment from the IzODA channel on the Anaconda cloud.  conda update  conda install bash=4.2  This step is necessary because updates are frequently made to the packages of the Anaconda root environment. The PID version of IzODA will usually be downlevel from the version that exists in the IzODA Anaconda channel. If you use Spark, you should also run the  conda install  to restore the version of bash that has been tested with Spark.",
            "title": "Post-SMP/E Installation Instructions"
        },
        {
            "location": "/anaconda/install-config/#activate-an-environment",
            "text": "A successful Anaconda installation results in a root environment that includes all of the packages shipped with IzODA. The root environment can be activated with the Anaconda  activate  command.  Note: as a temporary restriction, anaconda commands must be run from a bash sub-shell. If you have established a login with one bash session active, run bash again from the command line to create the necessary shell nesting before using Anaconda commands.  In order to affect the current shell environment, the Anaconda activate command must be run using the  source  bash builtin:  /home/condausr> bash  /home/condausr> source activate root  (root) /home/condausr>  Note that the name of the active Anaconda environment is in parentheses before any command line prompt that you have have set up. At this point, any packages in the active environment are available to applications that you may want to run.  In order to exit the root environment and return to you original shell, use the Anaconda  deactivate  command:  (root) /home/condausr> source deactivate root  /home/condausr>  A complete description of conda and its functions is available in the  conda User Guide .",
            "title": "Activate an Environment"
        },
        {
            "location": "/anaconda/install-config/#latebreaking-news",
            "text": "This section augments information in the  PSP bucket for Anaconda 1.1.0 .",
            "title": "Latebreaking News"
        },
        {
            "location": "/anaconda/install-config/#applying-maintenance",
            "text": "Anaconda takes a new approach to software maintenance. Rather than require all users on a system to use the same version of a given tool, Anaconda allows each user to select whatever version they wish to run, by controlling the content of their  conda environment . Users can create as many environments as they wish, potentially using different versions of tools for different task.  This means that installing new versions of the tools and packages controlled by conda  adds the new versions to the system without removing old versions . This allows users to transition to new versions on their own schedule.  That said, the  default  environment (also known as the  root  environment) is managed by the system administrator, and usually provides the most recent versions of tools.  The preferred means of installing new code into the conda root environment is by using the  conda update  command. This will obtain the latest versions of packages and tools from a  channel . IBM provides the  IzODA channel on the Anaconda cloud , which will be updated regularly with new packages, and new versions of existing packages. This channel is enabled by default when Anaconda is first installed.  However, not all installations will choose to install maintenance via the Internet. For such installations, IBM will provide regular PTFs for use with SMP/E.  Note, however, that installing maintenance via PTF will disable default access to the IzODA channel, under the presumption that subsequent maintenance will also be applied via SMP/E. Access can be restored by  altering the conda configuration file.",
            "title": "Applying maintenance"
        },
        {
            "location": "/anaconda/install-config/#maintaining-the-correct-file-ownership",
            "text": "If Anaconda is installed and maintained by a userid other than root, installation of maintenance will generally result in files in the Anaconda root directory having the owner and group IDs of that userid.  If, on the other hand, Anaconda is installed and maintained by root, files created or modified by installing maintenance (either via conda update or SMP/E) may produce files whose user and owner IDs are determined by the system that created the package. This is not desirable. To avoid this situation, execute the following commands, as root after installing maintenance:  cd /usr/lpp/IBM/izoda/anaconda   # or the Anaconda root directory, if changed\n chown -R 0:0 .  This will force the uid/gid for all of the files in the entire Anaconda root directory (and subdirectories) to 0, which is reserved for root.",
            "title": "Maintaining the correct file ownership"
        },
        {
            "location": "/anaconda/install-config/#terminology",
            "text": "root  The term \"root\" has one of three meanings, in the context of Anaconda: \n    root can mean the  root userid . This user has uid 0. However, in z/OS, there are other ways to get all privileges, and the use of uid of 0 is less common. \n    root can mean the  Anaconda root directory . This is usually /usr/lpp/IBM/izoda/anaconda. It can be changed at installation time, and will often be temporarily different while service is being applied. \n    root can mean the  root conda environment . When you run the script named activate to activate a conda environment, it sets an environment variable that causes bash to print the environment name in parenthesis. The default environment is named root.",
            "title": "Terminology"
        },
        {
            "location": "/anaconda/ivp-jupyter-notebook/",
            "text": "Anaconda/ODL Installation Verification Program (IVP) with Jupyter Notebook\n\n\n\nThis Installation Verification Program (IVP) is provided by IBM to get started with Anaconda and Optimized Data Layer (ODL) stacks of IzODA. Upon completion of this IVP, it ensures Anaconda and ODL have been installed successfully and users are able to run data visualizations and analysis on Mainframe data sources.\n\n\nBackground\n\n\nThe following shows step-by-step instructions on how to run IBM Open Data Analytics for z/OS Anaconda and ODL stacks. If you have not yet installed Anaconda including Python 3.6 for z/OS please do so using our \ninstallation and configuration page\n before proceeding.\n\n\nAnaconda for z/OS provides a \"conda\" command for managing packages and environments. It is similar to Anaconda for other platforms with the exception that it includes a different list of packages. However, many of the popular data science packages are included with this distribution, for instance, pandas, numpy, scipy, scikit-learn, matplotlib, seaborn, and more. In this IVP, you will create and run a Jupyter Notebook on an x86 platform with a Jupyter kernel on z/OS executing Python code using the \nHamlet\n package and \nJKG2AT's\n x86 install instructions. The Jupyter Notebook will demonstrate simple usage of data science packages by performing exploratory analysis on credit risk data retrieved from ODL. After completion of this IVP you will have learned the following:\n\n\n\nBasic conda commands\n\n\nJKG2AT and Hamlet setup\n\n\nSteps for executing code in a Jupyter Notebook\n\n\nIngesting data sources into ODL\n\n\nRetrieve data via the Python-ODL API (dsdbc)\n\n\nSimple data analysis on z/OS\n\n\n\n\nIngesting data into the Optimized Data Layer (ODL)\n\n\nThe data source we will be using is the \nGerman Credit Data\n from the UCI Machine Learning Repository. Please download the csv file titled \"ivp-german-data.csv\" \nhere.\n This data source is manipulated to avoid pre-processing such as, converting the input data into human-readable format, for the purpose of the IVP.\n\n\nFirst, we will convert the CSV data file into a mainframe data source and store it in ODL. ODL enables data from multiple, disconnected sources on z/OS to be virtually integrated into a single, logical data source, which can then be imported into a dataframe for further analysis. Note that previously, ODL was called Mainframe Data Services (MDS). We choose this setup as opposed to reading directly from csv to ensure that we can retrieve a data source from ODL via the ODL-Python module, dsdbc.\n\n\n   \nCreate a copybook to describe the data layout. Please use the copybook \nhere.\n\n   \nAllocate an empty data set on the host. For simplicity, create a physical sequential file with a record length that is wide enough to accommodate each record and call it \n\"CREDIT_DATA\"\n.\n\n   \nIn Data Server Studio, create a sequential virtual table using the copybook given above and the empty physical sequential dataset. We will use this dataset to insert our csv data.\n\n   \nUse the \nivp-load.py\n script to load the csv data into the PS dataset. The script uses pandas to read the csv data into a dataframe and dsdbc to insert the values in the dataframe into the physical dataset. Please change the ivp-load.py variable ssid to the subsystem id of the local data service server.\n\n\n\n\nOnce the script is done running, we now have the CSV data in ODL.\n\n\nFor more information on ODL please refer to the \nApache Spark Implementation on IBM z/OS\n\n\nNote:\n ODL is referred to as MDSS or Mainframe Data Service in the redbook. Since writing the redbook, it has changed names to Optimized Data Layer (ODL).\n\n\nHamlet and JKG2AT Setup\n\n\nTo execute and run a Jupyter Notebook server, the current solution is to use our \nHamlet\n conda package and \nJKG2AT's\n install process on x86.\n\n\nHamlet isn't included as part of the SMPE Anaconda install so you will need to run the command given in the IzODA channel:\n\n\nconda install -c izoda hamlet\n\n\nNote:\n Unless the system administrator set permissions for everyone to be able to administer Anaconda or if you want to install Hamlet in your own conda environment you will need to first make a clone of the environment:\n\n\nconda create -n <name-of-env> --clone=\"<path-to-anaconda-home>\"\n\n\nWhen the clone is complete, you will need to activate the new environment:\n\n\nsource activate <name-of-env>\n\n\nNotice that when you activate the new environment you will see the environment name in parenthesis before the prompt. After the clone is complete, please run the conda install command given above.\n\n\nWhen the install is complete, please run the following to ensure Hamlet is in the environment:\n\n\nconda list\n\n\nYou should see Hamlet listed as one of the installed conda packages:\n\n\nhamlet       1.0.0      6    izoda\n\n\nThe Hamlet install will also install its dependencies, apache toree and kernel-gateway among some others. Note for this IVP, we will only be using Jupyter's ipython kernel for python 3. Apache Toree kernel is used to interact with Apache Spark which will not be included in this IVP. After Hamlet is installed, please follow the instructions under \npath-to-anaconda-install-dir/pkgs/hamlet-1.0.0-0/info/recipe/README.md\n to setup Jupyter Kernel Gateway and the iPython kernel for Python 3 on z/OS. Again, you can ignore the Apache Toree setup explained in the instructions.\n\n\nOnce you have setup the z/OS side, you will now need to set up Jupyter Notebook Server on the x86 side. Please follow the instructions here.\n\n\nWhen both sides of the solution are setup, you should be able to start up jupyter kernel gateway using the following command:\n\n\njupyter kernelgateway\n\n\nNote:\n This is if the optional jupyter_kernel_gateway_config.py is setup. If not please run with --ip and --port options.\n\n\nAnd you should also be able to start up the notebook server on x86 in the container via the following run command:\n\n\n./start.sh <optional-config-file>\n\n\nYou will know the setup is successful if you see the jupyter notebook dashboard in your web browser:\n\n\n\n\nThen if you click on the drop-down menu \"New\" you should see Python 3 as one of the options for creating a new notebook.\n\n\nRunning Jupyter Notebook\n\n\nFor running the Jupyter Notebook, you will need to upload our Jupyter Notebook into your Jupyter Notebook server. In order to upload, please download \nCredit-Risk-Assessment-IVP.ipynb\n to your local machine. When you have finished downloading, click the \"Upload\" button in your Jupyter Notebook dashboard and find the .ipynb file you just downloaded. Then hit \"Open\". At this point you should see the notebook in your dashboard with the option to \"Upload\" next to the file name. Please click \"Upload\". Run the Jupyter Notebook by clicking on the .ipynb file. A new window should pop open and you'll see something like the following:\n\n\n\n\nNotice in the top right hand corner, it says the kernel you are using, in our case, Python 3. Also notice, the open circle that is next to the kernel name, this is telling us that the connection is established and the kernel is ready to execute code written in the Jupyter notebook cells (the different cells are denoted with \"In []:\" next to it) In our IVP notebook, there are two different types of cells, Markdown and code cells. Markdown cells include documentation for our IVP and code cells include python code to be executed by the jupyter kernel, ipython. To start executing code, click on a cell and go to \nCell > Run Cells\n (or use the shortcut keys Shift + Enter). The execution is done when you see a number in \"In []:\" i.e. \"In [1]:\" next to the cell. The number corresponds to the order the cell was run in. (Order matters! Please run the notebook from top to bottom or you can run all cells by going to \nCell > Run All\n). If there is a * within the brackets i.e. \"In [*]:\" that means the code is still executing. Note also that some of the code cells will produce output that will appear directly underneath the cell.\n\n\nThe Jupyter Notebook is split into three sections:\n\n\n\nRetrieve Data Source from ODL\n\n\nData Analysis with Pandas\n\n\nVisualization with Matplotlib\n\n\n\nIf all the code cells run successfully, you can ensure Anaconda has been installed properly. At this point, you should have a good understanding of how to use Jupyter Notebooks to perform data analysis/machine learning on mainframe data sources.\n\n\nAuthors: Yunli Tang, Joe Bostian    Date: September 5th, 2017",
            "title": "Anaconda/ODL Installation Verification with Jupyter Notebook"
        },
        {
            "location": "/anaconda/ivp-jupyter-notebook/#background",
            "text": "The following shows step-by-step instructions on how to run IBM Open Data Analytics for z/OS Anaconda and ODL stacks. If you have not yet installed Anaconda including Python 3.6 for z/OS please do so using our  installation and configuration page  before proceeding.  Anaconda for z/OS provides a \"conda\" command for managing packages and environments. It is similar to Anaconda for other platforms with the exception that it includes a different list of packages. However, many of the popular data science packages are included with this distribution, for instance, pandas, numpy, scipy, scikit-learn, matplotlib, seaborn, and more. In this IVP, you will create and run a Jupyter Notebook on an x86 platform with a Jupyter kernel on z/OS executing Python code using the  Hamlet  package and  JKG2AT's  x86 install instructions. The Jupyter Notebook will demonstrate simple usage of data science packages by performing exploratory analysis on credit risk data retrieved from ODL. After completion of this IVP you will have learned the following:  Basic conda commands  JKG2AT and Hamlet setup  Steps for executing code in a Jupyter Notebook  Ingesting data sources into ODL  Retrieve data via the Python-ODL API (dsdbc)  Simple data analysis on z/OS",
            "title": "Background"
        },
        {
            "location": "/anaconda/ivp-jupyter-notebook/#ingesting-data-into-the-optimized-data-layer-odl",
            "text": "The data source we will be using is the  German Credit Data  from the UCI Machine Learning Repository. Please download the csv file titled \"ivp-german-data.csv\"  here.  This data source is manipulated to avoid pre-processing such as, converting the input data into human-readable format, for the purpose of the IVP.  First, we will convert the CSV data file into a mainframe data source and store it in ODL. ODL enables data from multiple, disconnected sources on z/OS to be virtually integrated into a single, logical data source, which can then be imported into a dataframe for further analysis. Note that previously, ODL was called Mainframe Data Services (MDS). We choose this setup as opposed to reading directly from csv to ensure that we can retrieve a data source from ODL via the ODL-Python module, dsdbc. \n    Create a copybook to describe the data layout. Please use the copybook  here. \n    Allocate an empty data set on the host. For simplicity, create a physical sequential file with a record length that is wide enough to accommodate each record and call it  \"CREDIT_DATA\" . \n    In Data Server Studio, create a sequential virtual table using the copybook given above and the empty physical sequential dataset. We will use this dataset to insert our csv data. \n    Use the  ivp-load.py  script to load the csv data into the PS dataset. The script uses pandas to read the csv data into a dataframe and dsdbc to insert the values in the dataframe into the physical dataset. Please change the ivp-load.py variable ssid to the subsystem id of the local data service server.   Once the script is done running, we now have the CSV data in ODL.  For more information on ODL please refer to the  Apache Spark Implementation on IBM z/OS  Note:  ODL is referred to as MDSS or Mainframe Data Service in the redbook. Since writing the redbook, it has changed names to Optimized Data Layer (ODL).",
            "title": "Ingesting data into the Optimized Data Layer (ODL)"
        },
        {
            "location": "/anaconda/ivp-jupyter-notebook/#hamlet-and-jkg2at-setup",
            "text": "To execute and run a Jupyter Notebook server, the current solution is to use our  Hamlet  conda package and  JKG2AT's  install process on x86.  Hamlet isn't included as part of the SMPE Anaconda install so you will need to run the command given in the IzODA channel:  conda install -c izoda hamlet  Note:  Unless the system administrator set permissions for everyone to be able to administer Anaconda or if you want to install Hamlet in your own conda environment you will need to first make a clone of the environment:  conda create -n <name-of-env> --clone=\"<path-to-anaconda-home>\"  When the clone is complete, you will need to activate the new environment:  source activate <name-of-env>  Notice that when you activate the new environment you will see the environment name in parenthesis before the prompt. After the clone is complete, please run the conda install command given above.  When the install is complete, please run the following to ensure Hamlet is in the environment:  conda list  You should see Hamlet listed as one of the installed conda packages:  hamlet       1.0.0      6    izoda  The Hamlet install will also install its dependencies, apache toree and kernel-gateway among some others. Note for this IVP, we will only be using Jupyter's ipython kernel for python 3. Apache Toree kernel is used to interact with Apache Spark which will not be included in this IVP. After Hamlet is installed, please follow the instructions under  path-to-anaconda-install-dir/pkgs/hamlet-1.0.0-0/info/recipe/README.md  to setup Jupyter Kernel Gateway and the iPython kernel for Python 3 on z/OS. Again, you can ignore the Apache Toree setup explained in the instructions.  Once you have setup the z/OS side, you will now need to set up Jupyter Notebook Server on the x86 side. Please follow the instructions here.  When both sides of the solution are setup, you should be able to start up jupyter kernel gateway using the following command:  jupyter kernelgateway  Note:  This is if the optional jupyter_kernel_gateway_config.py is setup. If not please run with --ip and --port options.  And you should also be able to start up the notebook server on x86 in the container via the following run command:  ./start.sh <optional-config-file>  You will know the setup is successful if you see the jupyter notebook dashboard in your web browser:   Then if you click on the drop-down menu \"New\" you should see Python 3 as one of the options for creating a new notebook.",
            "title": "Hamlet and JKG2AT Setup"
        },
        {
            "location": "/anaconda/ivp-jupyter-notebook/#running-jupyter-notebook",
            "text": "For running the Jupyter Notebook, you will need to upload our Jupyter Notebook into your Jupyter Notebook server. In order to upload, please download  Credit-Risk-Assessment-IVP.ipynb  to your local machine. When you have finished downloading, click the \"Upload\" button in your Jupyter Notebook dashboard and find the .ipynb file you just downloaded. Then hit \"Open\". At this point you should see the notebook in your dashboard with the option to \"Upload\" next to the file name. Please click \"Upload\". Run the Jupyter Notebook by clicking on the .ipynb file. A new window should pop open and you'll see something like the following:   Notice in the top right hand corner, it says the kernel you are using, in our case, Python 3. Also notice, the open circle that is next to the kernel name, this is telling us that the connection is established and the kernel is ready to execute code written in the Jupyter notebook cells (the different cells are denoted with \"In []:\" next to it) In our IVP notebook, there are two different types of cells, Markdown and code cells. Markdown cells include documentation for our IVP and code cells include python code to be executed by the jupyter kernel, ipython. To start executing code, click on a cell and go to  Cell > Run Cells  (or use the shortcut keys Shift + Enter). The execution is done when you see a number in \"In []:\" i.e. \"In [1]:\" next to the cell. The number corresponds to the order the cell was run in. (Order matters! Please run the notebook from top to bottom or you can run all cells by going to  Cell > Run All ). If there is a * within the brackets i.e. \"In [*]:\" that means the code is still executing. Note also that some of the code cells will produce output that will appear directly underneath the cell.  The Jupyter Notebook is split into three sections:  Retrieve Data Source from ODL  Data Analysis with Pandas  Visualization with Matplotlib  \nIf all the code cells run successfully, you can ensure Anaconda has been installed properly. At this point, you should have a good understanding of how to use Jupyter Notebooks to perform data analysis/machine learning on mainframe data sources.  Authors: Yunli Tang, Joe Bostian    Date: September 5th, 2017",
            "title": "Running Jupyter Notebook"
        },
        {
            "location": "/anaconda/ivp-pyspark/",
            "text": "Anaconda/ODL Installation Verification Program (IVP) with PySpark\n\n\n\nThis Installation Verification Program (IVP) is provided by IBM to get started with the Anaconda and PySpark stacks of IzODA. Upon completion of this IVP, it ensures Anaconda and PySpark have been installed successfully and users are able to run simple data analysis on Mainframe data sources using Spark dataframes.\n\n\nThe following IVP utilizes IBM Open Data Analytics for z/OS Anaconda and PySpark stack. If you have not yet installed Anaconda including Python 3.6 for z/OS please do so using our \ninstallation and configuration page\n before proceeding.\n\n\nRunning the Program\n\n\nPlease download the python source code \nhere.\n To run the script please execute the following command:\n\n\n$SPARK_HOME/bin/spark-submit anaconda-pyspark-ivp.py [mdsURL] [user] [password]\n\n\nAuthors: Michael Gildein, Yunli Tang    Date: September 6th, 2017",
            "title": "Anaconda/ODL Installation Verification with PySpark"
        },
        {
            "location": "/anaconda/ivp-pyspark/#running-the-program",
            "text": "Please download the python source code  here.  To run the script please execute the following command:  $SPARK_HOME/bin/spark-submit anaconda-pyspark-ivp.py [mdsURL] [user] [password]  Authors: Michael Gildein, Yunli Tang    Date: September 6th, 2017",
            "title": "Running the Program"
        },
        {
            "location": "/ecosystem/",
            "text": "Ecosystem\n\n\n\nData analytics environments are cross-platform and hybrid by nature. Enterprise analytics environments don't generally exist in monolithic configurations.\n\n\nAlthough not a part of the offering, IzODA has an ecosystem of companion runtime capabilities that enable data scientists to develop analytics applications for the enterprise. These include popular interfaces, like Jupyter notebooks to drive workloads to z/OS-based analytics stacks like Spark and Anaconda/Python.\n\n\n\n\nThe IzODA team has created a set of configurations based on Jupyterhub and Jupyter Kernel Gateway that makes deployment of Jupyter notebook infrastructure easier. These configurations enable encrypted communication between the notebook server and z/OS host, and establish end user identity that allows better management of resource access an allocation on z/OS.\n\n\n\n\nSee the \nIzODA Jupyterhub\n github repo for downloads and a detailed description of this configuration.",
            "title": "Ecosystem"
        }
    ]
}