{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to IzODA's Documentation and Guides Sections are organized by components (Apache Spark, Optimized Data Layer/MDS, Anaconda, and Ecosystem). Documentation, Guides, and Demos can be found within their corresponding sections. Thank you for using IBM Open Data Analytics on z/OS. Release Notes The latest Release Notes can be found here. Important Links IzODA on IBM Knowledge Center IzODA Anaconda Package Channel IzODA Github Repository","title":"About"},{"location":"#welcome-to-izodas-documentation-and-guides","text":"Sections are organized by components (Apache Spark, Optimized Data Layer/MDS, Anaconda, and Ecosystem). Documentation, Guides, and Demos can be found within their corresponding sections. Thank you for using IBM Open Data Analytics on z/OS.","title":"Welcome to IzODA's Documentation and Guides"},{"location":"#release-notes","text":"The latest Release Notes can be found here.","title":"Release Notes"},{"location":"#important-links","text":"IzODA on IBM Knowledge Center IzODA Anaconda Package Channel IzODA Github Repository","title":"Important Links"},{"location":"ecosystem/","text":"Ecosystem Data analytics environments are cross-platform and hybrid by nature. Enterprise analytics environments don't generally exist in monolithic configurations. Although not a part of the offering, IzODA has an ecosystem of companion runtime capabilities that enable data scientists to develop analytics applications for the enterprise. These include popular interfaces, like Jupyter notebooks to drive workloads to z/OS-based analytics stacks like Spark and Anaconda/Python. The IzODA team has created a set of configurations based on Jupyterhub and Jupyter Kernel Gateway that makes deployment of Jupyter notebook infrastructure easier. These configurations enable encrypted communication between the notebook server and z/OS host, and establish end user identity that allows better management of resource access an allocation on z/OS. Visit the IzODA Jupyterhub Github repository for downloads and scroll down to the README for a detailed description of this configuration.","title":"Ecosystem"},{"location":"ecosystem/#ecosystem","text":"Data analytics environments are cross-platform and hybrid by nature. Enterprise analytics environments don't generally exist in monolithic configurations. Although not a part of the offering, IzODA has an ecosystem of companion runtime capabilities that enable data scientists to develop analytics applications for the enterprise. These include popular interfaces, like Jupyter notebooks to drive workloads to z/OS-based analytics stacks like Spark and Anaconda/Python. The IzODA team has created a set of configurations based on Jupyterhub and Jupyter Kernel Gateway that makes deployment of Jupyter notebook infrastructure easier. These configurations enable encrypted communication between the notebook server and z/OS host, and establish end user identity that allows better management of resource access an allocation on z/OS. Visit the IzODA Jupyterhub Github repository for downloads and scroll down to the README for a detailed description of this configuration.","title":"Ecosystem"},{"location":"anaconda/anaconda/","text":"Anaconda Anaconda is an assembly of parts that implement a Python-based analytics stack. It consists of multiple components: Python 3.7.0 / Python 3.6.1 The conda package manager A library of locally installed open source packages A channel, or reference repository located on the Anaconda Cloud Reference Architecture The environment above shows a user interacting with the Anaconda/Python stack using the Jupyter notebook ecosystem. Users may also perform analytics by running python applications directly from the command line of a shell session as they would any other python script. Installation and Customization Please refer to our Anaconda installation and configuration page. Verify Your Installation We have created a collection of small sample applications which you can run to verify that Anaconda is installed and configured properly. Please choose one of the following Install Verification Programs (IVPs) below to ensure your configuration is correct. IVP with ODL and Jupyter Notebook: This IVP demonstrates the Anaconda stack through a Jupyter Notebook and retrieves the data from ODL using a python module. IVP with Pyspark: This IVP demonstrates the Spark stack through a Python Jupyter notebook that uses Pyspark (Python API to Spark) to illustrate the use of Spark dataframes. Restrictions and Usage Notes There are some known issues with the Anaconda environment of IzODA. To avoid these issues, here is a list of restrictions on the functionality of Anaconda: Restrictions Mitigations The interactive backend packages for Matplotlib is not currently supported. This includes, GTK, WX, Cairo, etc. The user has the option instead, to use a non-interactive backend, capable of writing to a file. Use Matplotlib with the Seaborn backend, or use Bokeh for graphical visualization. The conda and python package levels must be kept in sync. Arbitrary combinations of conda and python package levels may result in errors. The supported conda and python version combinations are: conda (version 4.5.8, py37_5) and python (version 3.7.0, build 11) conda (version 4.5.8, build py36_5) and python (version 3.6.1, build 29) Distributed Dask capabilities are not yet supported. \u201cBig Data\u201d Dask collections can be used, and distributed capabilities will be made in beta form through the IzODA channel. Cannot write to HDF5 files using Dask\u2019s APIs. When creating an \"empty\" conda environment, be sure to include the Python package. Failing to do so can result in Python being unable to find its main shared library (libpython3.6m.so). If you have an environment active and install a new package into it, please exit and re-enter the environment using source deactivate/activate to ensure that all path variables in the environment are set properly. Usage Notes Several additional capabilities ship with Anaconda that have not yet been fully verified. These are not restrictions, but we advise that these are in beta form, and are not yet fully supported: Package installation from platform agnostic channels at anaconda.org Package installation using the Python Packaging Authority Installer (PIP) Jupyter notebook server on z/OS. We recommend continued use of our current Kernel Gateway/Apache Toree offering in concert with a Linux-based Jupyter notebook server. Anaconda users should run bash version 4.3.48 or later. Note: Apache Spark requires version 4.2.53. Recommended use is to make this the user's top level shell through the PROGRAM setting in the OMVS segment of their RACF profile. Using conda from arbitrarily nested shell sessions is not recommended. Some operations with the Dask package may result in error messages about broken pipes Work is continuing to resolve the problems underlying the restriction set. We will remove restrictions as they are resolved, and fully support the beta functions above as verification is completed. Migration Guides As more packages are updated, there can be intrusive changes to existing Anaconda environments that may require changes. Two of the known package migration changes are Apache Maven and JupyterHub to Jupyter Enterprise Gateway .","title":"Overview"},{"location":"anaconda/anaconda/#anaconda","text":"Anaconda is an assembly of parts that implement a Python-based analytics stack. It consists of multiple components: Python 3.7.0 / Python 3.6.1 The conda package manager A library of locally installed open source packages A channel, or reference repository located on the Anaconda Cloud","title":"Anaconda"},{"location":"anaconda/anaconda/#reference-architecture","text":"The environment above shows a user interacting with the Anaconda/Python stack using the Jupyter notebook ecosystem. Users may also perform analytics by running python applications directly from the command line of a shell session as they would any other python script.","title":"Reference Architecture"},{"location":"anaconda/anaconda/#installation-and-customization","text":"Please refer to our Anaconda installation and configuration page.","title":"Installation and Customization"},{"location":"anaconda/anaconda/#verify-your-installation","text":"We have created a collection of small sample applications which you can run to verify that Anaconda is installed and configured properly. Please choose one of the following Install Verification Programs (IVPs) below to ensure your configuration is correct. IVP with ODL and Jupyter Notebook: This IVP demonstrates the Anaconda stack through a Jupyter Notebook and retrieves the data from ODL using a python module. IVP with Pyspark: This IVP demonstrates the Spark stack through a Python Jupyter notebook that uses Pyspark (Python API to Spark) to illustrate the use of Spark dataframes.","title":"Verify Your Installation"},{"location":"anaconda/anaconda/#restrictions-and-usage-notes","text":"There are some known issues with the Anaconda environment of IzODA. To avoid these issues, here is a list of restrictions on the functionality of Anaconda: Restrictions Mitigations The interactive backend packages for Matplotlib is not currently supported. This includes, GTK, WX, Cairo, etc. The user has the option instead, to use a non-interactive backend, capable of writing to a file. Use Matplotlib with the Seaborn backend, or use Bokeh for graphical visualization. The conda and python package levels must be kept in sync. Arbitrary combinations of conda and python package levels may result in errors. The supported conda and python version combinations are: conda (version 4.5.8, py37_5) and python (version 3.7.0, build 11) conda (version 4.5.8, build py36_5) and python (version 3.6.1, build 29) Distributed Dask capabilities are not yet supported. \u201cBig Data\u201d Dask collections can be used, and distributed capabilities will be made in beta form through the IzODA channel. Cannot write to HDF5 files using Dask\u2019s APIs. When creating an \"empty\" conda environment, be sure to include the Python package. Failing to do so can result in Python being unable to find its main shared library (libpython3.6m.so). If you have an environment active and install a new package into it, please exit and re-enter the environment using source deactivate/activate to ensure that all path variables in the environment are set properly.","title":"Restrictions and Usage Notes"},{"location":"anaconda/anaconda/#usage-notes","text":"Several additional capabilities ship with Anaconda that have not yet been fully verified. These are not restrictions, but we advise that these are in beta form, and are not yet fully supported: Package installation from platform agnostic channels at anaconda.org Package installation using the Python Packaging Authority Installer (PIP) Jupyter notebook server on z/OS. We recommend continued use of our current Kernel Gateway/Apache Toree offering in concert with a Linux-based Jupyter notebook server. Anaconda users should run bash version 4.3.48 or later. Note: Apache Spark requires version 4.2.53. Recommended use is to make this the user's top level shell through the PROGRAM setting in the OMVS segment of their RACF profile. Using conda from arbitrarily nested shell sessions is not recommended. Some operations with the Dask package may result in error messages about broken pipes Work is continuing to resolve the problems underlying the restriction set. We will remove restrictions as they are resolved, and fully support the beta functions above as verification is completed.","title":"Usage Notes"},{"location":"anaconda/anaconda/#migration-guides","text":"As more packages are updated, there can be intrusive changes to existing Anaconda environments that may require changes. Two of the known package migration changes are Apache Maven and JupyterHub to Jupyter Enterprise Gateway .","title":"Migration Guides"},{"location":"anaconda/apar-notes/","text":"APAR Notes Q3 2018 - APAR # UI59973 Jupyter There are two new settings available for Jupyter to configure a port range: c.ConnectionFileMixin.starting_port and c.ConnectionFileMixin.max_kernels . Each kernel requires 5 consecutive ports so if you want to use 5 kernels you would need 25 consecutive ports. These two options are set in your Jupyter Notebook and Jupyter Kernel Gateway configuration files. The configuration files are located by default in ~/.jupyter . If you do not have configuration files, they can be generated with the following commands: jupyter notebook --generate-config and jupyter kernelgateway --generate-config","title":"APAR Notes"},{"location":"anaconda/apar-notes/#apar-notes","text":"","title":"APAR Notes"},{"location":"anaconda/apar-notes/#q3-2018-apar-ui59973-jupyter","text":"There are two new settings available for Jupyter to configure a port range: c.ConnectionFileMixin.starting_port and c.ConnectionFileMixin.max_kernels . Each kernel requires 5 consecutive ports so if you want to use 5 kernels you would need 25 consecutive ports. These two options are set in your Jupyter Notebook and Jupyter Kernel Gateway configuration files. The configuration files are located by default in ~/.jupyter . If you do not have configuration files, they can be generated with the following commands: jupyter notebook --generate-config and jupyter kernelgateway --generate-config","title":"Q3 2018 - APAR # UI59973 Jupyter"},{"location":"anaconda/install-config/","text":"Anaconda Installation and Configuration This process installs all of the components of a fully functional Anaconda deployment. There are some key characteristics of the z/OS environment to be aware of: All Anaconda components are 64-bit ASCII applications. Since Anaconda is an ASCII environment running on a platform with a default EBCDIC code page, file tagging is used extensively. Be aware that newly created files in this environment must be properly tagged for everything to run smoothly. These instructions are for a system administrator running in a z/OS shell. They are tailored for a bash shell instance. If you are using the default tcsh that comes with z/OS, substitute the setenv command for export . Proper WLM classification of Python workloads depends on how the tasks and processes associated with the application are named. By default, all such tasks will be associated with the name of the invoking user. However, this gives no indication to WLM that the given task is part of a Python workload. See the environmental setup section below for an explanation about how to associate a job name with the tasks of your Python application that WLM can recognize. Prerequisites The PSP bucket for Anaconda 1.1.0 should be reviewed prior to installation; see also the Latebreaking News section of this page. IzODA installs Anaconda from a local archive shipped as part of the SMP/E offering. Because of this, the install process requires room for the archive, plus about the same amount of space in the target file system that contains the Anaconda home directory - usually /usr/lpp/IBM/izoda/anaconda . Since Anaconda ships with approximately 295 packages, a finished install consumes between 2.5 and 3 GB. In addition to the disk space required during the IzODA install, it's important to remember that Anaconda keeps all package versions that have been installed from the IzODA channel through the conda command. This allows previous versions of an installed package to be referenced in addition to the latest version. Be sure to give Anaconda enough room to operate. The minimum disk space for an Anaconda installation is 6 GB, and the recommended amount is 20 GB. Python makes use of the /dev/urandom character special file to read pseudo-random values for use in a variety of applications. The Integrated Cryptographic Service Facility (ICSF) service of z/OS must be started for /dev/urandom to function properly. Failures will occur if ICSF is not active when Anaconda is installed. head -c40 /dev/urandom head: /dev/urandom: EDC5157I An internal error has occurred. The pre-requisite level of bash is 4.2.53. This is the version installed as the default during Anaconda installation. A 4.3.46 version of bash is also included as an Anaconda package that can be installed via conda. Environmental Setup This is the setup necessary for any user to run Python applications and make use of other Anaconda functions: Include this setup in your .bashrc profile to make file conversion and tagging transparent to your Python application: export _CEE_RUNOPTS=\"FILETAG(AUTOCVT,AUTOTAG) POSIX(ON)\" if [[ -z \u201d$_BPXK_AUTOCVT\"]]; then export _BPXK_AUTOCVT=ON exec $BASH \"$@\" fi Add the Anaconda bin directory to your PATH in .bashrc: export ANACONDA_ROOT=\"/usr/lpp/IBM/izoda/anaconda\" export PATH=$ANACONDA_ROOT/bin:$PATH The Anaconda bin directory doesn't have to be first in your PATH, but if there are other shells or versions of bash installed on the system, the Anaconda bin directory has to preceed them in the PATH. Note: substitute the actual path to the Anaconda root from your environment when exporting ANACONDA_ROOT. Most Python applications will use the Optimized Data Layer to access data sources on z/OS and from remote servers. These call to ODL through the dsdbc api. Set up your STEPLIB to include the load library for this interface in .bashrc: export STEPLIB=hlq.SAZKLOAD:$STEPLIB where hlq is the high-level qualifier of the ODL (MDS) installation. - Grant users running Python applications READ permission to the BPX.JOBNAME facility class profile so that they can assign jobnames to their workload. Please see the _BPX environment variables section of the z/OS UNIX System Services Planning guide for more information. Note: superusers are allowed to set jobnames regardless of their access to BPX.JOBNAME. In general, we recommend against Python users having superuser privileges. Download and Unpack IzODA installs using SMP/E. Instructions are in the Program Directory. The PSP bucket for Anaconda 1.1.0 should be reviewed prior to installation; see also the Latebreaking News section of this page. Install The SMP/E jobs need to be run in order. Each job includes its own instructions at the top. Post-SMP/E Installation Instructions Anaconda requires a post-install script be run after the SMP/e installation has been completed. This script known as the Anaconda configuration script, must be run after service is applied to Anaconda via SMP/e. The script does the following: Creates a \"local file channel\" in the Anaconda root directory (if it does not already exist). This channel will contain all of the Anaconda packages delivered via SMP/E. Updates the local file channel to contain the most recently delivered Anaconda packages. Ensures that the file ownership and permission attributes are correct for all of the installed Anaconda packages. Updates the Anaconda \"root environment\" to contain the most recently delivered Anaconda packages. Creates an alternative environment in the Anaconda root directory the corresponds to the updated Anaconda root environment. The first time the script is run, it also creates alternative environments that correspond to the previously delivered service levels. These alternative environments can be used by Anaconda users to run their code using packages that correspond to service levels other than the default (\"root\") level. Performs any other release-specific actions necessary to keep the Anaconda system functioning properly. The following steps must be followed before running the script: Log into Unix System Services with a Userid with \"superuser\" authority and become the superuser by typing the su command. Navigate to the Anaconda root directory. i.e. /usr/lpp/IBM/izoda/anaconda cd /usr/lpp/IBM/izoda/anaconda Create a deployment_prefix file by issuing the following command: ./configure-anaconda --set-prefix /usr/lpp/IBM/izoda/anaconda Configure Anaconda and install packages delivered by SMP/e: ./configure-anaconda Note, depending on how many maintenance releases were installed, this step may take an hour or two. Activate an Environment A successful Anaconda installation results in a root environment that includes all of the packages shipped with IzODA. The root environment can be activated with the Anaconda activate command. Note: as a temporary restriction, anaconda commands must be run from a bash sub-shell. If you have established a login with one bash session active, run bash again from the command line to create the necessary shell nesting before using Anaconda commands. In order to affect the current shell environment, the Anaconda activate command must be run using the source bash built in: /home/condausr> bash /home/condausr> source activate root (root) /home/condausr> Note that the name of the active Anaconda environment is in parentheses before any command line prompt that you have have set up. At this point, any packages in the active environment are available to applications that you may want to run. In order to exit the root environment and return to you original shell, use the Anaconda deactivate command: (root) /home/condausr> source deactivate root /home/condausr> A complete description of conda and its functions is available in the conda User Guide . Latebreaking News This section augments information in the PSP bucket for Anaconda 1.1.0 . Applying maintenance Anaconda takes a new approach to software maintenance. Rather than require all users on a system to use the same version of a given tool, Anaconda allows each user to select whatever version they wish to run, by controlling the content of their *conda environment. Users can create as many environments as they wish, potentially using different versions of tools for different task. This means that installing new versions of the tools and packages controlled by conda adds the new versions to the system without removing old versions . This allows users to transition to new versions on their own schedule. That said, the default environment (also known as the root environment) is managed by the system administrator, and usually provides the most recent versions of tools. The preferred means of installing new code into the conda root environment is by using the conda update command. This will obtain the latest versions of packages and tools from a channel . IBM provides the IzODA channel on the Anaconda cloud , which will be updated regularly with new packages, and new versions of existing packages. This channel is enabled by default when Anaconda is first installed. However, not all installations will choose to install maintenance via the Internet. For such installations, IBM will provide regular PTFs for use with SMP/E. Note, however, that installing maintenance via PTF will disable default access to the IzODA channel, under the presumption that subsequent maintenance will also be applied via SMP/E. Access can be restored by altering the conda configuration file. Maintaining the correct file ownership If Anaconda is installed and maintained by a userid other than root, installation of maintenance will generally result in files in the Anaconda root directory having the owner and group IDs of that userid. If, on the other hand, Anaconda is installed and maintained by root, files created or modified by installing maintenance (either via conda update or SMP/E) may produce files whose user and owner IDs are determined by the system that created the package. This is not desirable. To avoid this situation, execute the following commands, as root after installing maintenance: cd /usr/lpp/IBM/izoda/anaconda # or the Anaconda root directory, if changed chown -R 0:0 This will force the uid/gid for all of the files in the entire Anaconda root directory (and subdirectories) to 0, which is reserved for root. Terminology root The term root has one of three meanings, in the context of Anaconda: root can mean the root userid . This user has uid 0. However, in z/OS, there are other ways to get all privileges, and the use of uid of 0 is less common root can mean the Anaconda root directory . This is usually /usr/lpp/IBM/izoda/anaconda. It can be changed at installation time, and will often be temporarily different while service is being applied. root can mean the root conda environment . When you run the script named activate to activate a conda environment, it sets an environment variable that causes bash to print the environment name in parenthesis. The default environment is named root.","title":"Installation and Configuration"},{"location":"anaconda/install-config/#anaconda-installation-and-configuration","text":"This process installs all of the components of a fully functional Anaconda deployment. There are some key characteristics of the z/OS environment to be aware of: All Anaconda components are 64-bit ASCII applications. Since Anaconda is an ASCII environment running on a platform with a default EBCDIC code page, file tagging is used extensively. Be aware that newly created files in this environment must be properly tagged for everything to run smoothly. These instructions are for a system administrator running in a z/OS shell. They are tailored for a bash shell instance. If you are using the default tcsh that comes with z/OS, substitute the setenv command for export . Proper WLM classification of Python workloads depends on how the tasks and processes associated with the application are named. By default, all such tasks will be associated with the name of the invoking user. However, this gives no indication to WLM that the given task is part of a Python workload. See the environmental setup section below for an explanation about how to associate a job name with the tasks of your Python application that WLM can recognize.","title":"Anaconda Installation and Configuration"},{"location":"anaconda/install-config/#prerequisites","text":"The PSP bucket for Anaconda 1.1.0 should be reviewed prior to installation; see also the Latebreaking News section of this page. IzODA installs Anaconda from a local archive shipped as part of the SMP/E offering. Because of this, the install process requires room for the archive, plus about the same amount of space in the target file system that contains the Anaconda home directory - usually /usr/lpp/IBM/izoda/anaconda . Since Anaconda ships with approximately 295 packages, a finished install consumes between 2.5 and 3 GB. In addition to the disk space required during the IzODA install, it's important to remember that Anaconda keeps all package versions that have been installed from the IzODA channel through the conda command. This allows previous versions of an installed package to be referenced in addition to the latest version. Be sure to give Anaconda enough room to operate. The minimum disk space for an Anaconda installation is 6 GB, and the recommended amount is 20 GB. Python makes use of the /dev/urandom character special file to read pseudo-random values for use in a variety of applications. The Integrated Cryptographic Service Facility (ICSF) service of z/OS must be started for /dev/urandom to function properly. Failures will occur if ICSF is not active when Anaconda is installed. head -c40 /dev/urandom head: /dev/urandom: EDC5157I An internal error has occurred. The pre-requisite level of bash is 4.2.53. This is the version installed as the default during Anaconda installation. A 4.3.46 version of bash is also included as an Anaconda package that can be installed via conda.","title":"Prerequisites"},{"location":"anaconda/install-config/#environmental-setup","text":"This is the setup necessary for any user to run Python applications and make use of other Anaconda functions: Include this setup in your .bashrc profile to make file conversion and tagging transparent to your Python application: export _CEE_RUNOPTS=\"FILETAG(AUTOCVT,AUTOTAG) POSIX(ON)\" if [[ -z \u201d$_BPXK_AUTOCVT\"]]; then export _BPXK_AUTOCVT=ON exec $BASH \"$@\" fi Add the Anaconda bin directory to your PATH in .bashrc: export ANACONDA_ROOT=\"/usr/lpp/IBM/izoda/anaconda\" export PATH=$ANACONDA_ROOT/bin:$PATH The Anaconda bin directory doesn't have to be first in your PATH, but if there are other shells or versions of bash installed on the system, the Anaconda bin directory has to preceed them in the PATH. Note: substitute the actual path to the Anaconda root from your environment when exporting ANACONDA_ROOT. Most Python applications will use the Optimized Data Layer to access data sources on z/OS and from remote servers. These call to ODL through the dsdbc api. Set up your STEPLIB to include the load library for this interface in .bashrc: export STEPLIB=hlq.SAZKLOAD:$STEPLIB where hlq is the high-level qualifier of the ODL (MDS) installation. - Grant users running Python applications READ permission to the BPX.JOBNAME facility class profile so that they can assign jobnames to their workload. Please see the _BPX environment variables section of the z/OS UNIX System Services Planning guide for more information. Note: superusers are allowed to set jobnames regardless of their access to BPX.JOBNAME. In general, we recommend against Python users having superuser privileges.","title":"Environmental Setup"},{"location":"anaconda/install-config/#download-and-unpack","text":"IzODA installs using SMP/E. Instructions are in the Program Directory. The PSP bucket for Anaconda 1.1.0 should be reviewed prior to installation; see also the Latebreaking News section of this page.","title":"Download and Unpack"},{"location":"anaconda/install-config/#install","text":"The SMP/E jobs need to be run in order. Each job includes its own instructions at the top.","title":"Install"},{"location":"anaconda/install-config/#post-smpe-installation-instructions","text":"Anaconda requires a post-install script be run after the SMP/e installation has been completed. This script known as the Anaconda configuration script, must be run after service is applied to Anaconda via SMP/e. The script does the following: Creates a \"local file channel\" in the Anaconda root directory (if it does not already exist). This channel will contain all of the Anaconda packages delivered via SMP/E. Updates the local file channel to contain the most recently delivered Anaconda packages. Ensures that the file ownership and permission attributes are correct for all of the installed Anaconda packages. Updates the Anaconda \"root environment\" to contain the most recently delivered Anaconda packages. Creates an alternative environment in the Anaconda root directory the corresponds to the updated Anaconda root environment. The first time the script is run, it also creates alternative environments that correspond to the previously delivered service levels. These alternative environments can be used by Anaconda users to run their code using packages that correspond to service levels other than the default (\"root\") level. Performs any other release-specific actions necessary to keep the Anaconda system functioning properly. The following steps must be followed before running the script: Log into Unix System Services with a Userid with \"superuser\" authority and become the superuser by typing the su command. Navigate to the Anaconda root directory. i.e. /usr/lpp/IBM/izoda/anaconda cd /usr/lpp/IBM/izoda/anaconda Create a deployment_prefix file by issuing the following command: ./configure-anaconda --set-prefix /usr/lpp/IBM/izoda/anaconda Configure Anaconda and install packages delivered by SMP/e: ./configure-anaconda Note, depending on how many maintenance releases were installed, this step may take an hour or two.","title":"Post-SMP/E Installation Instructions"},{"location":"anaconda/install-config/#activate-an-environment","text":"A successful Anaconda installation results in a root environment that includes all of the packages shipped with IzODA. The root environment can be activated with the Anaconda activate command. Note: as a temporary restriction, anaconda commands must be run from a bash sub-shell. If you have established a login with one bash session active, run bash again from the command line to create the necessary shell nesting before using Anaconda commands. In order to affect the current shell environment, the Anaconda activate command must be run using the source bash built in: /home/condausr> bash /home/condausr> source activate root (root) /home/condausr> Note that the name of the active Anaconda environment is in parentheses before any command line prompt that you have have set up. At this point, any packages in the active environment are available to applications that you may want to run. In order to exit the root environment and return to you original shell, use the Anaconda deactivate command: (root) /home/condausr> source deactivate root /home/condausr> A complete description of conda and its functions is available in the conda User Guide .","title":"Activate an Environment"},{"location":"anaconda/install-config/#latebreaking-news","text":"This section augments information in the PSP bucket for Anaconda 1.1.0 .","title":"Latebreaking News"},{"location":"anaconda/install-config/#applying-maintenance","text":"Anaconda takes a new approach to software maintenance. Rather than require all users on a system to use the same version of a given tool, Anaconda allows each user to select whatever version they wish to run, by controlling the content of their *conda environment. Users can create as many environments as they wish, potentially using different versions of tools for different task. This means that installing new versions of the tools and packages controlled by conda adds the new versions to the system without removing old versions . This allows users to transition to new versions on their own schedule. That said, the default environment (also known as the root environment) is managed by the system administrator, and usually provides the most recent versions of tools. The preferred means of installing new code into the conda root environment is by using the conda update command. This will obtain the latest versions of packages and tools from a channel . IBM provides the IzODA channel on the Anaconda cloud , which will be updated regularly with new packages, and new versions of existing packages. This channel is enabled by default when Anaconda is first installed. However, not all installations will choose to install maintenance via the Internet. For such installations, IBM will provide regular PTFs for use with SMP/E. Note, however, that installing maintenance via PTF will disable default access to the IzODA channel, under the presumption that subsequent maintenance will also be applied via SMP/E. Access can be restored by altering the conda configuration file.","title":"Applying maintenance"},{"location":"anaconda/install-config/#maintaining-the-correct-file-ownership","text":"If Anaconda is installed and maintained by a userid other than root, installation of maintenance will generally result in files in the Anaconda root directory having the owner and group IDs of that userid. If, on the other hand, Anaconda is installed and maintained by root, files created or modified by installing maintenance (either via conda update or SMP/E) may produce files whose user and owner IDs are determined by the system that created the package. This is not desirable. To avoid this situation, execute the following commands, as root after installing maintenance: cd /usr/lpp/IBM/izoda/anaconda # or the Anaconda root directory, if changed chown -R 0:0 This will force the uid/gid for all of the files in the entire Anaconda root directory (and subdirectories) to 0, which is reserved for root.","title":"Maintaining the correct file ownership"},{"location":"anaconda/install-config/#terminology","text":"root The term root has one of three meanings, in the context of Anaconda: root can mean the root userid . This user has uid 0. However, in z/OS, there are other ways to get all privileges, and the use of uid of 0 is less common root can mean the Anaconda root directory . This is usually /usr/lpp/IBM/izoda/anaconda. It can be changed at installation time, and will often be temporarily different while service is being applied. root can mean the root conda environment . When you run the script named activate to activate a conda environment, it sets an environment variable that causes bash to print the environment name in parenthesis. The default environment is named root.","title":"Terminology"},{"location":"anaconda/ivp-jupyter-notebook/","text":"Anaconda/ODL Installation Verification Program (IVP) with Jupyter Notebook Jupyter Kernel Gateway with NB2KG To execute and run a Jupyter Notebook server, the current solution is to use our Jupyter Kernelgateway(JKG) on z/OS and NB2KG's install process on x86. In general, we highly recommend users creating their own Conda Environments. This allows each user to have full access to the packages they intend to use. To create a conda environment you can use the following command conda create -n <environment name> python This will create a new environment with python and its dependencies in ~/.conda/envs. This environment can then be activated with one of the following commands source activate <environment name> Or conda activate <environment name> You can confirm the environment has been activated with one of two ways, if you run conda info \u2013envs all of your environments will be listed, and the current active one will have a \u201c*\u201d next to it. Or if you look at your shell you will see your environments name in parenthesis preceding your input. Once you have your environment activated we will need to install a few packages through the conda package manager, mainly the dependencies for creating a python kernel and running JKG or JEG. The command to perform any install with conda is conda install . So in this case lets do conda install jupyter-kernel-gateway From here we will need to create a python 3 kernel for jupyter to use. This can be accomplished with the following command python -m ipykernel install \u2013user After this point you should configure JKG. You can generate a config file with the following command jupyter kernelgateway --generate-config There are a few values that will need to be edited and uncommented. They are the following #c.KernelGatewayApp.allow_origin = '*' #c.JupyterWebsocketPersonality.list_kernels = True #c.KernelGatewayApp.ip = '<ip>' #c.KernelGatewayApp.port = <port> This will conclude the configuration on z/OS and you should move onto configuring NB2KG on an x86 system. It\u2019s instructions can be found in the NB2KG link above or here . When both sides of the solution are setup, you should be able to start up jupyter kernel gateway using the following command: jupyter kernelgateway Note: This is if the optional jupyter_kernel_gateway_config.py is setup. If not please run with --ip and --port options. And you should also be able to start up the notebook server on x86 in the container via the following run command: ./start.sh <optional-config-file> You will know the setup is successful if you see the jupyter notebook dashboard in your web browser: Then if you click on the drop-down menu \"New\" you should see Python 3 as one of the options for creating a new notebook. Running Jupyter Notebook For running the Jupyter Notebook, you will need to upload our Jupyter Notebook into your Jupyter Notebook server. In order to upload, please download Credit-Risk-Assessment-IVP.ipynb to your local machine. When you have finished downloading, click the \"Upload\" button in your Jupyter Notebook dashboard and find the .ipynb file you just downloaded. Then hit \"Open\". At this point you should see the notebook in your dashboard with the option to \"Upload\" next to the file name. Please click \"Upload\". Run the Jupyter Notebook by clicking on the .ipynb file. A new window should pop open and you'll see something like the following: Notice in the top right hand corner, it says the kernel you are using, in our case, Python 3. Also notice, the open circle that is next to the kernel name, this is telling us that the connection is established and the kernel is ready to execute code written in the Jupyter notebook cells (the different cells are denoted with \"In []:\" next to it) In our IVP notebook, there are two different types of cells, Markdown and code cells. Markdown cells include documentation for our IVP and code cells include python code to be executed by the jupyter kernel, ipython. To start executing code, click on a cell and go to Cell > Run Cells (or use the shortcut keys Shift + Enter). The execution is done when you see a number in \"In []:\" i.e. \"In [1]:\" next to the cell. The number corresponds to the order the cell was run in. (Order matters! Please run the notebook from top to bottom or you can run all cells by going to Cell > Run All ). If there is a * within the brackets i.e. \"In [*]:\" that means the code is still executing. Note also that some of the code cells will produce output that will appear directly underneath the cell. The Jupyter Notebook is split into three sections: Retrieve Data Source from ODL Data Analysis with Pandas Visualization with Matplotlib If all the code cells run successfully, you can ensure Anaconda has been installed properly. At this point, you should have a good understanding of how to use Jupyter Notebooks to perform data analysis/machine learning on mainframe data sources. Authors: Yunli Tang, Joe Bostian Date: September 5th, 2017","title":"Anaconda/ODL Installation Verification with Jupyter Notebook"},{"location":"anaconda/ivp-jupyter-notebook/#anacondaodl-installation-verification-program-ivp-with-jupyter-notebook","text":"","title":"Anaconda/ODL Installation Verification Program (IVP) with Jupyter Notebook"},{"location":"anaconda/ivp-jupyter-notebook/#jupyter-kernel-gateway-with-nb2kg","text":"To execute and run a Jupyter Notebook server, the current solution is to use our Jupyter Kernelgateway(JKG) on z/OS and NB2KG's install process on x86. In general, we highly recommend users creating their own Conda Environments. This allows each user to have full access to the packages they intend to use. To create a conda environment you can use the following command conda create -n <environment name> python This will create a new environment with python and its dependencies in ~/.conda/envs. This environment can then be activated with one of the following commands source activate <environment name> Or conda activate <environment name> You can confirm the environment has been activated with one of two ways, if you run conda info \u2013envs all of your environments will be listed, and the current active one will have a \u201c*\u201d next to it. Or if you look at your shell you will see your environments name in parenthesis preceding your input. Once you have your environment activated we will need to install a few packages through the conda package manager, mainly the dependencies for creating a python kernel and running JKG or JEG. The command to perform any install with conda is conda install . So in this case lets do conda install jupyter-kernel-gateway From here we will need to create a python 3 kernel for jupyter to use. This can be accomplished with the following command python -m ipykernel install \u2013user After this point you should configure JKG. You can generate a config file with the following command jupyter kernelgateway --generate-config There are a few values that will need to be edited and uncommented. They are the following #c.KernelGatewayApp.allow_origin = '*' #c.JupyterWebsocketPersonality.list_kernels = True #c.KernelGatewayApp.ip = '<ip>' #c.KernelGatewayApp.port = <port> This will conclude the configuration on z/OS and you should move onto configuring NB2KG on an x86 system. It\u2019s instructions can be found in the NB2KG link above or here . When both sides of the solution are setup, you should be able to start up jupyter kernel gateway using the following command: jupyter kernelgateway Note: This is if the optional jupyter_kernel_gateway_config.py is setup. If not please run with --ip and --port options. And you should also be able to start up the notebook server on x86 in the container via the following run command: ./start.sh <optional-config-file> You will know the setup is successful if you see the jupyter notebook dashboard in your web browser: Then if you click on the drop-down menu \"New\" you should see Python 3 as one of the options for creating a new notebook.","title":"Jupyter Kernel Gateway with NB2KG"},{"location":"anaconda/ivp-jupyter-notebook/#running-jupyter-notebook","text":"For running the Jupyter Notebook, you will need to upload our Jupyter Notebook into your Jupyter Notebook server. In order to upload, please download Credit-Risk-Assessment-IVP.ipynb to your local machine. When you have finished downloading, click the \"Upload\" button in your Jupyter Notebook dashboard and find the .ipynb file you just downloaded. Then hit \"Open\". At this point you should see the notebook in your dashboard with the option to \"Upload\" next to the file name. Please click \"Upload\". Run the Jupyter Notebook by clicking on the .ipynb file. A new window should pop open and you'll see something like the following: Notice in the top right hand corner, it says the kernel you are using, in our case, Python 3. Also notice, the open circle that is next to the kernel name, this is telling us that the connection is established and the kernel is ready to execute code written in the Jupyter notebook cells (the different cells are denoted with \"In []:\" next to it) In our IVP notebook, there are two different types of cells, Markdown and code cells. Markdown cells include documentation for our IVP and code cells include python code to be executed by the jupyter kernel, ipython. To start executing code, click on a cell and go to Cell > Run Cells (or use the shortcut keys Shift + Enter). The execution is done when you see a number in \"In []:\" i.e. \"In [1]:\" next to the cell. The number corresponds to the order the cell was run in. (Order matters! Please run the notebook from top to bottom or you can run all cells by going to Cell > Run All ). If there is a * within the brackets i.e. \"In [*]:\" that means the code is still executing. Note also that some of the code cells will produce output that will appear directly underneath the cell. The Jupyter Notebook is split into three sections: Retrieve Data Source from ODL Data Analysis with Pandas Visualization with Matplotlib If all the code cells run successfully, you can ensure Anaconda has been installed properly. At this point, you should have a good understanding of how to use Jupyter Notebooks to perform data analysis/machine learning on mainframe data sources. Authors: Yunli Tang, Joe Bostian Date: September 5th, 2017","title":"Running Jupyter Notebook"},{"location":"anaconda/ivp-pyspark/","text":"IzODA Installation Verification Program (IVP) with PySpark This Installation Verification Program (IVP) is provided by IBM to get started with the Anaconda and PySpark stacks of IzODA. Upon completion of this IVP, it ensures Anaconda and PySpark have been installed successfully and users are able to run simple data analysis on Mainframe data sources using Spark dataframes. The following IVP utilizes IBM Open Data Analytics for z/OS Anaconda and PySpark stack. If you have not yet installed Anaconda including Python 3.6 for z/OS please do so using our installation and configuration page before proceeding. Running the Program Please download the python source code here. To run the script please execute the following command: $SPARK_HOME/bin/spark-submit anaconda-pyspark-ivp.py \"[mdsURL]\" Few things to note: * In order for this file to run successfully it must be in ASCII encoding. * mdsURL should look similar to jdbc:rs:dv://$HOST:$PORT;DBTY=DVS , where $HOST:$PORT is the url for your MDS server. * Parameter should have double-quotes around it. If PySpark is not found, you need to export Python path as follows: export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\" Authors: Michael Gildein, Yunli Tang, Connor Hayes Date: September 11th, 2019","title":"IzODA Installation Verification with PySpark"},{"location":"anaconda/ivp-pyspark/#izoda-installation-verification-program-ivp-with-pyspark","text":"This Installation Verification Program (IVP) is provided by IBM to get started with the Anaconda and PySpark stacks of IzODA. Upon completion of this IVP, it ensures Anaconda and PySpark have been installed successfully and users are able to run simple data analysis on Mainframe data sources using Spark dataframes. The following IVP utilizes IBM Open Data Analytics for z/OS Anaconda and PySpark stack. If you have not yet installed Anaconda including Python 3.6 for z/OS please do so using our installation and configuration page before proceeding.","title":"IzODA Installation Verification Program (IVP) with PySpark"},{"location":"anaconda/ivp-pyspark/#running-the-program","text":"Please download the python source code here. To run the script please execute the following command: $SPARK_HOME/bin/spark-submit anaconda-pyspark-ivp.py \"[mdsURL]\" Few things to note: * In order for this file to run successfully it must be in ASCII encoding. * mdsURL should look similar to jdbc:rs:dv://$HOST:$PORT;DBTY=DVS , where $HOST:$PORT is the url for your MDS server. * Parameter should have double-quotes around it. If PySpark is not found, you need to export Python path as follows: export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\" Authors: Michael Gildein, Yunli Tang, Connor Hayes Date: September 11th, 2019","title":"Running the Program"},{"location":"anaconda/izoda-extras/","text":"izoda-extras After adding Apache Livy to the IzODA platform, an issue was found in which non-Python applications were installing into the root directory of environments instead of in their own dedicated directories. A solution was found in which the izoda-extras package is installed first, which creates an extras directory, then all non-Python applications will install inside the extras directory. Note: The non-Python applications that get installed here will need to be added to your path with this command: export PATH=$CONDA_PREFIX/extras/$PACKAGE_NAME/bin:$PATH","title":"izoda-extras"},{"location":"anaconda/izoda-extras/#izoda-extras","text":"After adding Apache Livy to the IzODA platform, an issue was found in which non-Python applications were installing into the root directory of environments instead of in their own dedicated directories. A solution was found in which the izoda-extras package is installed first, which creates an extras directory, then all non-Python applications will install inside the extras directory. Note: The non-Python applications that get installed here will need to be added to your path with this command: export PATH=$CONDA_PREFIX/extras/$PACKAGE_NAME/bin:$PATH","title":"izoda-extras"},{"location":"anaconda/jkg2at-migration/","text":"JKG2AT Migration Guide JKG2AT will be deprecated. All current users will be forced to uninstall jkg2at to upgrade Toree. JKG2AT functionality is now built into the newer versions of Toree and Jupyter Kernel Gateway so it has become irrelevant. Steps to take to upgrade your current environment. conda remove jkg2at conda install toree conda install jupyter-kernel-gateway jupyter toree install \u2013user Then if you are using the ATTLS support laid out in jkg2at you will still need to make the .so file within jupyter-kernel-gateway program controlled. This file is located in $CONDA_PREFIX/lib/python3.X/site-packages/kernelgateway The above ATTLS support is only in versions of Jupyter Kernel Gateway before 2.3.x","title":"JKG2AT Migration"},{"location":"anaconda/jkg2at-migration/#jkg2at-migration-guide","text":"JKG2AT will be deprecated. All current users will be forced to uninstall jkg2at to upgrade Toree. JKG2AT functionality is now built into the newer versions of Toree and Jupyter Kernel Gateway so it has become irrelevant. Steps to take to upgrade your current environment. conda remove jkg2at conda install toree conda install jupyter-kernel-gateway jupyter toree install \u2013user Then if you are using the ATTLS support laid out in jkg2at you will still need to make the .so file within jupyter-kernel-gateway program controlled. This file is located in $CONDA_PREFIX/lib/python3.X/site-packages/kernelgateway The above ATTLS support is only in versions of Jupyter Kernel Gateway before 2.3.x","title":"JKG2AT Migration Guide"},{"location":"anaconda/optional-packages/","text":"Optional Package Installation Guides izoda-extras izoda-extras will create a new directory $CONDA_PREFIX/extras. Non-Python optional packages will install at $CONDA_PREFIX/extras/$PACKAGE_NAME . To use one of these packages at it's new location, it will need to be added to the PATH by adding export PATH=$CONDA_PREFIX/extras/$PACKAGE_NAME/bin:$PATH to the user's bash profile. For more information, see izoda-extras . Installing in the Base Environment Please see Modifying the Base Anaconda Environment . Apache Maven conda install apache-maven export PATH=$CONDA_PREFIX/extras/apache-maven/bin:$PATH Apache Livy Refer to the Installation and Customization Guide Bash 4.3 conda install bash=4.3","title":"Optional Package Guides"},{"location":"anaconda/optional-packages/#optional-package-installation-guides","text":"","title":"Optional Package Installation Guides"},{"location":"anaconda/optional-packages/#izoda-extras","text":"izoda-extras will create a new directory $CONDA_PREFIX/extras. Non-Python optional packages will install at $CONDA_PREFIX/extras/$PACKAGE_NAME . To use one of these packages at it's new location, it will need to be added to the PATH by adding export PATH=$CONDA_PREFIX/extras/$PACKAGE_NAME/bin:$PATH to the user's bash profile. For more information, see izoda-extras .","title":"izoda-extras"},{"location":"anaconda/optional-packages/#installing-in-the-base-environment","text":"Please see Modifying the Base Anaconda Environment .","title":"Installing in the Base Environment"},{"location":"anaconda/optional-packages/#apache-maven","text":"conda install apache-maven export PATH=$CONDA_PREFIX/extras/apache-maven/bin:$PATH","title":"Apache Maven"},{"location":"anaconda/optional-packages/#apache-livy","text":"Refer to the Installation and Customization Guide","title":"Apache Livy"},{"location":"anaconda/optional-packages/#bash-43","text":"conda install bash=4.3","title":"Bash 4.3"},{"location":"anaconda/packages/","text":"IzODA Anaconda Packages The following are the packages provided by the IzODA Anaconda Channel. Label/Release Level: main Language / Tool: Select Python R-lang Tool Generate Environment Script","title":"Available Packages"},{"location":"anaconda/packages/#izoda-anaconda-packages","text":"The following are the packages provided by the IzODA Anaconda Channel. Label/Release Level: main Language / Tool: Select Python R-lang Tool Generate Environment Script","title":"IzODA Anaconda Packages"},{"location":"anaconda/r-jupyter-notebook/","text":"Using R in a Jupyter Notebook To run a Jupyter Notebook with R, you need to create a conda environment and activate the kernel so Jupyter can recognize it. Then you can work with the R language in a notebook. Create a new conda environment: conda create -n r-kernel Activate the environment: conda activate r-kernel Install the R-kernel and Jupyter packages: conda install r-recommended r-irkernel conda install Jupyter Add the R-kernel to Jupyter by installing a kernel spec. This allows Jupyter to recognize the kernel and work with it interactively: R -e 'IRkernel::installspec()' Run the Jupyter notebook: jupyter-notebook Copy the generated URL and paste it into a browser. Be careful to leave the appropriate machine name. Click the New drop-down button and select R . When the notebook launches, enter and run your code.","title":"Using R in Jupyter Notebook"},{"location":"anaconda/r-jupyter-notebook/#using-r-in-a-jupyter-notebook","text":"To run a Jupyter Notebook with R, you need to create a conda environment and activate the kernel so Jupyter can recognize it. Then you can work with the R language in a notebook. Create a new conda environment: conda create -n r-kernel Activate the environment: conda activate r-kernel Install the R-kernel and Jupyter packages: conda install r-recommended r-irkernel conda install Jupyter Add the R-kernel to Jupyter by installing a kernel spec. This allows Jupyter to recognize the kernel and work with it interactively: R -e 'IRkernel::installspec()' Run the Jupyter notebook: jupyter-notebook Copy the generated URL and paste it into a browser. Be careful to leave the appropriate machine name. Click the New drop-down button and select R . When the notebook launches, enter and run your code.","title":"Using R in a Jupyter Notebook"},{"location":"best-practices/hlq-c-libraries/","text":"PYAN-1432 HLQ for C Libraries Issue 08/06/2019 Note : This is an issue that is currently being worked on. Using anaconda in anaconda/bin/xlc and anaconda/bin/xlc.cfg , there is a specific HLQ for C libraries. If a user does not have the same HLQ, xlc will fail with an error similar to this: IKJ56228I DATA SET CEE.SCEELKEX NOT IN CATALOG OR CATALOG CAN NOT BE ACCESSED FSUM3052 The data definition name SYSLIB cannot be resolved. The data set was not found. Ensure that data set name CEE.SCEELKEX is specified correctly. Ideally something like this should be handled by a system programmer and should be made visible to them in the hold data and other forms of documentation such as IzODA's github pages.","title":"HLQ xlc for C Libraries"},{"location":"best-practices/hlq-c-libraries/#pyan-1432-hlq-for-c-libraries-issue","text":"","title":"PYAN-1432 HLQ for C Libraries Issue"},{"location":"best-practices/hlq-c-libraries/#08062019","text":"Note : This is an issue that is currently being worked on. Using anaconda in anaconda/bin/xlc and anaconda/bin/xlc.cfg , there is a specific HLQ for C libraries. If a user does not have the same HLQ, xlc will fail with an error similar to this: IKJ56228I DATA SET CEE.SCEELKEX NOT IN CATALOG OR CATALOG CAN NOT BE ACCESSED FSUM3052 The data definition name SYSLIB cannot be resolved. The data set was not found. Ensure that data set name CEE.SCEELKEX is specified correctly. Ideally something like this should be handled by a system programmer and should be made visible to them in the hold data and other forms of documentation such as IzODA's github pages.","title":"08/06/2019"},{"location":"best-practices/jupyter-notebook-usage/","text":"Jupyter Notebook Usage When using Jupyter Notebook on z/OS you should generate a config file with the command jupyter notebook --generate-config This will create a notebook config file in ~/.jupyter/jupyter_notebook_config.py . If you want a basic Jupyter Notebook config file you can start with this c.NotebookApp.open_browser = False c.NotebookApp.ip = '<IP>' c.NotebookApp.port = <Port> If you would like to use Jupyter Notebook with z/OS authentication you should have a config file that looks like the one below. One more step is required to enable z/OS authentication. You will need to navigate to your anaconda root directory and run install_set_program_control c.NotebookApp.open_browser = False c.NotebookApp.ip = '<IP>' c.NotebookApp.port = <Port> c.NotebookApp.username = '<z/OS User ID> FAQ: Without specifying the IP address in the config file Jupyter will default to use localhost The IP you set is where Jupyter will accept requests from An IP of 0.0.0.0 will accept requests from everywhere Jupyter Notebook will default to token authentication","title":"Jupyter Notebook Usage"},{"location":"best-practices/jupyter-notebook-usage/#jupyter-notebook-usage","text":"When using Jupyter Notebook on z/OS you should generate a config file with the command jupyter notebook --generate-config This will create a notebook config file in ~/.jupyter/jupyter_notebook_config.py . If you want a basic Jupyter Notebook config file you can start with this c.NotebookApp.open_browser = False c.NotebookApp.ip = '<IP>' c.NotebookApp.port = <Port> If you would like to use Jupyter Notebook with z/OS authentication you should have a config file that looks like the one below. One more step is required to enable z/OS authentication. You will need to navigate to your anaconda root directory and run install_set_program_control c.NotebookApp.open_browser = False c.NotebookApp.ip = '<IP>' c.NotebookApp.port = <Port> c.NotebookApp.username = '<z/OS User ID> FAQ: Without specifying the IP address in the config file Jupyter will default to use localhost The IP you set is where Jupyter will accept requests from An IP of 0.0.0.0 will accept requests from everywhere Jupyter Notebook will default to token authentication","title":"Jupyter Notebook Usage"},{"location":"best-practices/modify-base-env/","text":"Modifying the Base Anaconda Environment If modifying the base Anaconda environment instead of creating new environments, please ensure the following are set correctly: Note: Ensure you are logged in as the superuser that installed Anaconda in the past. Permissions Determine the proper permissions are set by issuing: ls -la /usr/lpp/IBM/izoda/anaconda/extras Set the umask by issuing: umask #### User and Group After installation of the IzODA Extras packages, it has been found that the UID and GID for /usr/lpp/IBM/izoda/anaconda/extras may be set incorrectly. To fix this: - Determine if the proper owner and group is set by issuing: ls -la /usr/lpp/IBM/izoda/anaconda/extras - If necessary, change the owner and group by issuing: chown -Rh UID:GID /usr/lpp/IBM/izoda/anaconda/extras","title":"Modifying the Base Environment"},{"location":"best-practices/modify-base-env/#modifying-the-base-anaconda-environment","text":"If modifying the base Anaconda environment instead of creating new environments, please ensure the following are set correctly: Note: Ensure you are logged in as the superuser that installed Anaconda in the past.","title":"Modifying the Base Anaconda Environment"},{"location":"best-practices/modify-base-env/#permissions","text":"Determine the proper permissions are set by issuing: ls -la /usr/lpp/IBM/izoda/anaconda/extras Set the umask by issuing: umask ####","title":"Permissions"},{"location":"best-practices/modify-base-env/#user-and-group","text":"After installation of the IzODA Extras packages, it has been found that the UID and GID for /usr/lpp/IBM/izoda/anaconda/extras may be set incorrectly. To fix this: - Determine if the proper owner and group is set by issuing: ls -la /usr/lpp/IBM/izoda/anaconda/extras - If necessary, change the owner and group by issuing: chown -Rh UID:GID /usr/lpp/IBM/izoda/anaconda/extras","title":"User and Group"},{"location":"best-practices/vegas-support/","text":"Vegas Support In Q32019 Toree will include that jars for Vegas. Documentation and usage notes on Vegas can be found on their Github Repository here https://github.com/vegas-viz/Vegas . Please note using Vegas will require a connection to the internet.","title":"Vegas Support"},{"location":"best-practices/vegas-support/#vegas-support","text":"In Q32019 Toree will include that jars for Vegas. Documentation and usage notes on Vegas can be found on their Github Repository here https://github.com/vegas-viz/Vegas . Please note using Vegas will require a connection to the internet.","title":"Vegas Support"},{"location":"migration/ODL_mig/","text":"Optimized Data Layer The Optimized Data Layer is also known as the IBM Mainframe Data Service for Apache Spark (MDS, or MDSS). It is a started task in the z/OS address space that provides a common interface to several different mainframe and off-platform data sources. The kinds of data sources that can be accessed through ODL include: IBM DB2 IBM Virtual Storage Access Method (VSAM) IBM Information Management System (IMS) Partitioned Data Sets (PDS) z/OS sequential files (including log files) ADABAS Data from other platforms: Databases - SQL and NoSQL (e.g. MongoDB, Oracle, Teradata) Streaming feeds (e.g. Twitter, Facebook) Access to these data sources is provided by: JDBC for Java and Scala applications, usually run from the Spark environment. dsdbc for Python applications running in the Anaconda environment. Although ODL was originally developed to provide a JDBC interface to Scala and Java applications that use the Spark stack, it has been extended for IzODA to service Python applications through the dsdbc interface. Examples of both types of data access can be found in our examples Github repo. ODL includes a workstation based interface called the Data Service Studio (DSS) that is used to create, view, and change metadata describing the mapping from mainframe data sources to a relational view of the data on provided by ODL on z/OS. DSS is based on Eclipse. It can either be installed separately, or can be added to an existing Eclipse shell. For more details about the z/OS and workstation-based components of ODL and how they relate to one another, as well as details about alternative methods for virtualizing database views, please see: New Product The replacement for IzODA ODL is a full license for the \"IBM Data Virtualization Manager\" offering. You can learn about these products here , here and here . Migration Instructions for migrating from IzODA ODL to full-license DVM can be found here (internal). Instructions for migrating from IzODA ODL to full-license DVM can be found here (external) Usage Notes None yet.","title":"ODL/MDS"},{"location":"migration/ODL_mig/#optimized-data-layer","text":"The Optimized Data Layer is also known as the IBM Mainframe Data Service for Apache Spark (MDS, or MDSS). It is a started task in the z/OS address space that provides a common interface to several different mainframe and off-platform data sources. The kinds of data sources that can be accessed through ODL include: IBM DB2 IBM Virtual Storage Access Method (VSAM) IBM Information Management System (IMS) Partitioned Data Sets (PDS) z/OS sequential files (including log files) ADABAS Data from other platforms: Databases - SQL and NoSQL (e.g. MongoDB, Oracle, Teradata) Streaming feeds (e.g. Twitter, Facebook) Access to these data sources is provided by: JDBC for Java and Scala applications, usually run from the Spark environment. dsdbc for Python applications running in the Anaconda environment. Although ODL was originally developed to provide a JDBC interface to Scala and Java applications that use the Spark stack, it has been extended for IzODA to service Python applications through the dsdbc interface. Examples of both types of data access can be found in our examples Github repo. ODL includes a workstation based interface called the Data Service Studio (DSS) that is used to create, view, and change metadata describing the mapping from mainframe data sources to a relational view of the data on provided by ODL on z/OS. DSS is based on Eclipse. It can either be installed separately, or can be added to an existing Eclipse shell. For more details about the z/OS and workstation-based components of ODL and how they relate to one another, as well as details about alternative methods for virtualizing database views, please see:","title":"Optimized Data Layer"},{"location":"migration/ODL_mig/#new-product","text":"The replacement for IzODA ODL is a full license for the \"IBM Data Virtualization Manager\" offering. You can learn about these products here , here and here .","title":"New Product"},{"location":"migration/ODL_mig/#migration","text":"Instructions for migrating from IzODA ODL to full-license DVM can be found here (internal). Instructions for migrating from IzODA ODL to full-license DVM can be found here (external)","title":"Migration"},{"location":"migration/ODL_mig/#usage-notes","text":"None yet.","title":"Usage Notes"},{"location":"migration/anaconda_mig/","text":"Anaconda Anaconda is an assembly of parts that implement a Python-based analytics stack. It consists of multiple components: Python 3.7.0 / Python 3.6.1 The conda package manager A library of locally installed open source packages A channel, or reference repository located on the Anaconda Cloud New Product The replacement for IzODA Anaconda and Python is the IBM \"IBM Enterprise Python\" offering, with the Python AI Toolkit. You can learn about these products here , here and here . Migration TBD Use xyz to print out packages installed on Anaconda Delete all environments Create a virtual environment using IBM Enterprise Python Use PIP to install packages from Python and Python AI Toolkit. Steps to migrate from IzODA Anaconda to Enterprise Python and PAIT Friday, February 24, 2023 2:16 PM Install and configure IBM Enterprise Python according to documented instructions Locate and install a copy of BASH. You may use the open source version from Rocket to obtain Bash 4.3, the ZOSOpenTools version at https://github.com/ZOSOpenTools/bashport/releases for Bash 5.2. You can continue to use the bash in the IzODA directory, until securing another copy. Make a note of packages used in your current Anaconda environment Clean your conda environment with `conda clean --all` List environments created in conda cat \\~/.conda/environments.txt For each environment, list the packages used: conda activate tempEnv1 Conda list Delete the environment. Conda deactivate Conda remove -n tempEnv1 \u2013all Edit your .bashrc to remove references to Conda, `/usr/lpp/IBM/anaconda` Note: You may want to leave references to /usr/lpp/IBM/anaconda/bin in the PATH variable if you need to retain access to BASH until a non-IzODA copy is obtained. How does /usr/bin/bash-4.3 affect this? Time to start configuring your new Python and PIP environment! Use `python3 -m ensurepip --upgrade` to ensure PIP installed Use `python3 -m pip install --upgrade pip` to get latest PIP Add the Python AI Toolkit to your environment Follow the \"set up environment\" instructions. You will be creating a `$HOME/.config/pip/pip.conf` using the following commands: touch $HOME/.config/pip/pip.conf chtag -t -c ISO8859-1 $HOME/.config/pip/pip.conf Create a virtual environment for holding your packages: (Hint: this creates a directory for future use. You should be in your $HOME directory or a directory under it, and not under the .config directory.) python3 -m venv theFirst source python_play/bin/activate Install packages with `python3 -m pip install <package name> (See the available list of Python AI toolkit packages at: https://ibm-z-oss-oda.github.io/python_ai_toolkit_zos/ ) Using \u201cbeautifulsoup4\u201d as an example: Usage Notes None yet.","title":"Anaconda"},{"location":"migration/anaconda_mig/#anaconda","text":"Anaconda is an assembly of parts that implement a Python-based analytics stack. It consists of multiple components: Python 3.7.0 / Python 3.6.1 The conda package manager A library of locally installed open source packages A channel, or reference repository located on the Anaconda Cloud","title":"Anaconda"},{"location":"migration/anaconda_mig/#new-product","text":"The replacement for IzODA Anaconda and Python is the IBM \"IBM Enterprise Python\" offering, with the Python AI Toolkit. You can learn about these products here , here and here .","title":"New Product"},{"location":"migration/anaconda_mig/#migration","text":"TBD Use xyz to print out packages installed on Anaconda Delete all environments Create a virtual environment using IBM Enterprise Python Use PIP to install packages from Python and Python AI Toolkit. Steps to migrate from IzODA Anaconda to Enterprise Python and PAIT Friday, February 24, 2023 2:16 PM Install and configure IBM Enterprise Python according to documented instructions Locate and install a copy of BASH. You may use the open source version from Rocket to obtain Bash 4.3, the ZOSOpenTools version at https://github.com/ZOSOpenTools/bashport/releases for Bash 5.2. You can continue to use the bash in the IzODA directory, until securing another copy. Make a note of packages used in your current Anaconda environment Clean your conda environment with `conda clean --all` List environments created in conda cat \\~/.conda/environments.txt For each environment, list the packages used: conda activate tempEnv1 Conda list Delete the environment. Conda deactivate Conda remove -n tempEnv1 \u2013all Edit your .bashrc to remove references to Conda, `/usr/lpp/IBM/anaconda` Note: You may want to leave references to /usr/lpp/IBM/anaconda/bin in the PATH variable if you need to retain access to BASH until a non-IzODA copy is obtained. How does /usr/bin/bash-4.3 affect this? Time to start configuring your new Python and PIP environment! Use `python3 -m ensurepip --upgrade` to ensure PIP installed Use `python3 -m pip install --upgrade pip` to get latest PIP Add the Python AI Toolkit to your environment Follow the \"set up environment\" instructions. You will be creating a `$HOME/.config/pip/pip.conf` using the following commands: touch $HOME/.config/pip/pip.conf chtag -t -c ISO8859-1 $HOME/.config/pip/pip.conf Create a virtual environment for holding your packages: (Hint: this creates a directory for future use. You should be in your $HOME directory or a directory under it, and not under the .config directory.) python3 -m venv theFirst source python_play/bin/activate Install packages with `python3 -m pip install <package name> (See the available list of Python AI toolkit packages at: https://ibm-z-oss-oda.github.io/python_ai_toolkit_zos/ ) Using \u201cbeautifulsoup4\u201d as an example:","title":"Migration"},{"location":"migration/anaconda_mig/#usage-notes","text":"None yet.","title":"Usage Notes"},{"location":"migration/overview/","text":"Overview The IBM Open Data Analytics for z/OS (IzODA) product has reached End of Marketing as of March 7, 2023, so customers must migrate to the replacement offerings. You can read about the new Open Data Analytics initiative on the IBM Journey to Open Data Analytics Content Solution Page Where IzODA was ordered with one PID (product ID) from ShopZ, there are now several PIDs that can be ordered to select which components of IzODA are installed. This is more fully explained in this blog . Once the desired new products have been ordered and installed, your configurations will need to be migrated to the new product(s). You can find migration documentation, as follows: Migrating from Anaconda to IBM Enterprise Python and the Python AI Toolkit Migrating from IzODA Spark 2.4 to IBM Z Platform for Apache Spark (Spark 3.2) Migrating from IzODA ODL to IBM Data Virtualization Manager Note that the Bash shell, formerly provided with IzODA Anaconda, must be obtained separately. The following versions have been shown to work: - Rocket Open tools - IBM ZOpen tools The following functions have no replacement in the new Open Data Analytics product set: - zOSMF Dynamic Workflow for configuring Spark - Apache Livy","title":"Overview"},{"location":"migration/overview/#overview","text":"The IBM Open Data Analytics for z/OS (IzODA) product has reached End of Marketing as of March 7, 2023, so customers must migrate to the replacement offerings. You can read about the new Open Data Analytics initiative on the IBM Journey to Open Data Analytics Content Solution Page Where IzODA was ordered with one PID (product ID) from ShopZ, there are now several PIDs that can be ordered to select which components of IzODA are installed. This is more fully explained in this blog . Once the desired new products have been ordered and installed, your configurations will need to be migrated to the new product(s). You can find migration documentation, as follows: Migrating from Anaconda to IBM Enterprise Python and the Python AI Toolkit Migrating from IzODA Spark 2.4 to IBM Z Platform for Apache Spark (Spark 3.2) Migrating from IzODA ODL to IBM Data Virtualization Manager Note that the Bash shell, formerly provided with IzODA Anaconda, must be obtained separately. The following versions have been shown to work: - Rocket Open tools - IBM ZOpen tools The following functions have no replacement in the new Open Data Analytics product set: - zOSMF Dynamic Workflow for configuring Spark - Apache Livy","title":"Overview"},{"location":"migration/spark_mig/","text":"Apache Spark Apache Spark is a general purpose, high performance clustering analytics engine that allocates resources and distributes work across a set of processes and tasks. It is organized as a set core functions that underpin a collection of functional libraries. Spark was originally released through the IBM z/OS Platform for Apache Spark offering in March of 2016. New Product The replacement for IzODA Apache Spark is the IBM Z Platform for Apache Spark (PID 5698-SPK). You can learn about this product here , here and here . Migration Steps to migrate from IzODA Spark (Spark 2.4) to new Spark product (Spark 3.x) Read through the entirety of these instructions before starting. Knowing the information needed in later steps can often inform and improve the decisions made in the earlier steps. Obtain and install the IBM Z Platform for Apache Spark product (5698-SPK) Notable differences between IzODA Spark and ZSpark It should install to /usr/lpp/IBM/zspark/ with a resulting SPARK_HOME directory of /usr/lpp/IBM/zspark/spark/spark32x Sample started task JCL is availabel within the SPARK_HOME directory, in $SPARK_HOME/samples/zos/jcl, instead of a separate SAMPLIB data set. The product component prefix was \"AZK\" and is now \"AFK\", the component ID is HSPK130 (rather than HSPK120). This version uses Log4J V2. The configuration options for Log4J2 are set in $SPARK_CONF_DIR/log4j2.properties, which can be copied from $SPARK_HOME/conf/log4j2.properties.template Product documentation can be found at https://www.ibm.com/docs/en/zpas/1.1.0 Choose whether to use the existing configuration options and logging locations, or create a new installation with new directories, new port numbers assigned, new Spark job names, new Spark \"SPARKID\" master user, new RACF resource names (some RACF updates will be required, regardless.) Considerations: Upgrading in place is probably easiest, but will require users to update their configuration and applications (recompile) after the cluster master/worker is updated. It will also be more difficult to test without affecting end users. More on this below. If you used the existing naming convention for Spark started tasks (e.g AZKMSTR, etc.), this may become confusing with the new spark product. The Spark z/OS component prefix of \"AZK\" for IzODA Spark, and is now AFK for IBM Z Platform for Apache Spark. If upgrading in place and reusing the directories (conf, logs, work, etc.) and port numbers To-do: started task procedure updates Make a backup copy of existing Spark started task procedures (e.g. AZKMSTR, AZKHIST, AZKWRKR) Using TSO OGET or ISPF cut/paste, copy the started task JCL for AFKMSTR, AFKWRKR and (optionally)AFKHIST from `$SPARK_HOME/samples/zos/jcl` to your PROCLIB data set. Example (using ISPF option 6) ISPF Command Shell ISPF Command ===> Enter TSO or Workstation commands below: ===> oget '/usr/lpp/IBM/zspark/spark/spark32x/samples/zos/jcl/afkhist.jcl' 'sbj.zspark.test.proclib(afkhist)' text The samples are shipped in EBCDIC, so you should not need the CONVERT keyword on the TSO OGET. The TEXT keyword is also unneeded, but added here for clarity. Repeat for the other two included sample PROCLIB JCL members. Disable external shuffle server by removing it from COMMNDxx or other automation. (In Apache Spark 3.x, the shuffle server will be part of the Spark Worker in all supported IBM Z Platform for Apache Spark MVS environments.) Update the Spark configuration Locate the SPARK_CONF_DIR setting for the existing IzODA Spark installation. Update the \"export SPARK_CONF_DIR=\" statement in each of the JCL procedures to point to your installation's Spark conf directory. Update SPARK_HOME environment variable (See 1.a.i above) in the $SPARK_CONF_DIR/spark-zos-started-tasks.sh file, if necessary. As noted above, you must use the new `log4j2.properties` file which has a different format than in log4j v1. To do this, copy the template file to your Spark conf directory. For example, if your installation's Spark conf directory is /etc/zspark/spark/conf, you could copy the file using the following USS command: cp $SPARK_HOME/conf/log4j2.properties.template /etc/zspark/spark/conf/ If you have made any changes to the Spark \"log4j.properties\" file, you will need to make those same changes to the new file, adjusting for the syntax changes. See the following reference for details: Reference: https://stackoverflow.com/questions/35900555/migrating-from-log4j-to-log4j2-properties-file-configuration . z/OS updates Since you are reusing the IzODA Spark configuration, Spark configuration is done. However, TCPIP and RACF updates might be needed. TCPIP Port adjustments Many customers \"lock down\" their TCPIP ports, only allowing specific jobnames to access specific TCPIP port numbers. If the spark jobnames for the master, worker or history server have changed, updates to the TCPIP PORTS files to include those job names may be necessary DO NOT use the SHAREPORT keyword on IzODA Spark or IBM Z Platform for Apache Spark port specifications. This is documented in the troubleshooting section of the I & C Guide. ATTLS ? Trusted Partner ? SAF updates Add the new ZSpark SAF resource. For RACF, this would be: RDEFINE XFACILIT AFK.SPARK.MASTER.CONNECT UACC(NONE) PERMIT AFK.SPARK.MASTER.CONNECT ID(SPARKUSR) CLASS(XFACILIT) + ACC(READ) If you created new started task procs, you will need to associate those with the SPARKID userid: RDEFINE STARTED PENZMSTR.* STDATA(USER(SPARKID) GROUP(SYS1)) RDEFINE STARTED PENZWRKR.* STDATA(USER(SPARKID) GROUP(SYS1)) RDEFINE STARTED PENZHIST.* STDATA(USER(SPARKID) GROUP(SYS1)) If creating a new Spark installation, use the book and do all that stuff. The steps above will help you locate the changes you made to IzODA environment to properly configure zSpark, as documented in the I & C Guide. IVT Regardless of whether you reused your customized IzODA Spark configuration or created a new Spark configuration, you can use the steps documented in Chapter 5. Verifying the IBM Z Platform for Apache Spark customization of the IBM Z Platform for Apache Spark Installation and Customization Guide (GI13-5809-00) User updates: Inform the user of the new product and configuration details. They will need to update their environment variables including SPARK_HOME, SPARK_CONF_DIR, logs, IBM Z Platform for Apache Spark features a different version of Spark than IzODA Spark, Spark version 3.2. To use the Spark platform, users will need to recompile their application with Spark 3.2 classes. In addition, if you are also converting from IzODA MDS to DVM, users will need to procure the appropriate DVM JDBC driver to access DVM. If any virtual tables are renamed, they will need that information to update the parameters being passed to their JDBC driver in their code. See <ref to DVM migration guide or pubs here.>","title":"Apache Spark"},{"location":"migration/spark_mig/#apache-spark","text":"Apache Spark is a general purpose, high performance clustering analytics engine that allocates resources and distributes work across a set of processes and tasks. It is organized as a set core functions that underpin a collection of functional libraries. Spark was originally released through the IBM z/OS Platform for Apache Spark offering in March of 2016.","title":"Apache Spark"},{"location":"migration/spark_mig/#new-product","text":"The replacement for IzODA Apache Spark is the IBM Z Platform for Apache Spark (PID 5698-SPK). You can learn about this product here , here and here .","title":"New Product"},{"location":"migration/spark_mig/#migration","text":"","title":"Migration"},{"location":"migration/spark_mig/#steps-to-migrate-from-izoda-spark-spark-24-to-new-spark-product-spark-3x","text":"Read through the entirety of these instructions before starting. Knowing the information needed in later steps can often inform and improve the decisions made in the earlier steps. Obtain and install the IBM Z Platform for Apache Spark product (5698-SPK) Notable differences between IzODA Spark and ZSpark It should install to /usr/lpp/IBM/zspark/ with a resulting SPARK_HOME directory of /usr/lpp/IBM/zspark/spark/spark32x Sample started task JCL is availabel within the SPARK_HOME directory, in $SPARK_HOME/samples/zos/jcl, instead of a separate SAMPLIB data set. The product component prefix was \"AZK\" and is now \"AFK\", the component ID is HSPK130 (rather than HSPK120). This version uses Log4J V2. The configuration options for Log4J2 are set in $SPARK_CONF_DIR/log4j2.properties, which can be copied from $SPARK_HOME/conf/log4j2.properties.template Product documentation can be found at https://www.ibm.com/docs/en/zpas/1.1.0 Choose whether to use the existing configuration options and logging locations, or create a new installation with new directories, new port numbers assigned, new Spark job names, new Spark \"SPARKID\" master user, new RACF resource names (some RACF updates will be required, regardless.) Considerations: Upgrading in place is probably easiest, but will require users to update their configuration and applications (recompile) after the cluster master/worker is updated. It will also be more difficult to test without affecting end users. More on this below. If you used the existing naming convention for Spark started tasks (e.g AZKMSTR, etc.), this may become confusing with the new spark product. The Spark z/OS component prefix of \"AZK\" for IzODA Spark, and is now AFK for IBM Z Platform for Apache Spark. If upgrading in place and reusing the directories (conf, logs, work, etc.) and port numbers To-do: started task procedure updates Make a backup copy of existing Spark started task procedures (e.g. AZKMSTR, AZKHIST, AZKWRKR) Using TSO OGET or ISPF cut/paste, copy the started task JCL for AFKMSTR, AFKWRKR and (optionally)AFKHIST from `$SPARK_HOME/samples/zos/jcl` to your PROCLIB data set. Example (using ISPF option 6) ISPF Command Shell ISPF Command ===> Enter TSO or Workstation commands below: ===> oget '/usr/lpp/IBM/zspark/spark/spark32x/samples/zos/jcl/afkhist.jcl' 'sbj.zspark.test.proclib(afkhist)' text The samples are shipped in EBCDIC, so you should not need the CONVERT keyword on the TSO OGET. The TEXT keyword is also unneeded, but added here for clarity. Repeat for the other two included sample PROCLIB JCL members. Disable external shuffle server by removing it from COMMNDxx or other automation. (In Apache Spark 3.x, the shuffle server will be part of the Spark Worker in all supported IBM Z Platform for Apache Spark MVS environments.) Update the Spark configuration Locate the SPARK_CONF_DIR setting for the existing IzODA Spark installation. Update the \"export SPARK_CONF_DIR=\" statement in each of the JCL procedures to point to your installation's Spark conf directory. Update SPARK_HOME environment variable (See 1.a.i above) in the $SPARK_CONF_DIR/spark-zos-started-tasks.sh file, if necessary. As noted above, you must use the new `log4j2.properties` file which has a different format than in log4j v1. To do this, copy the template file to your Spark conf directory. For example, if your installation's Spark conf directory is /etc/zspark/spark/conf, you could copy the file using the following USS command: cp $SPARK_HOME/conf/log4j2.properties.template /etc/zspark/spark/conf/ If you have made any changes to the Spark \"log4j.properties\" file, you will need to make those same changes to the new file, adjusting for the syntax changes. See the following reference for details: Reference: https://stackoverflow.com/questions/35900555/migrating-from-log4j-to-log4j2-properties-file-configuration . z/OS updates Since you are reusing the IzODA Spark configuration, Spark configuration is done. However, TCPIP and RACF updates might be needed. TCPIP Port adjustments Many customers \"lock down\" their TCPIP ports, only allowing specific jobnames to access specific TCPIP port numbers. If the spark jobnames for the master, worker or history server have changed, updates to the TCPIP PORTS files to include those job names may be necessary DO NOT use the SHAREPORT keyword on IzODA Spark or IBM Z Platform for Apache Spark port specifications. This is documented in the troubleshooting section of the I & C Guide. ATTLS ? Trusted Partner ? SAF updates Add the new ZSpark SAF resource. For RACF, this would be: RDEFINE XFACILIT AFK.SPARK.MASTER.CONNECT UACC(NONE) PERMIT AFK.SPARK.MASTER.CONNECT ID(SPARKUSR) CLASS(XFACILIT) + ACC(READ) If you created new started task procs, you will need to associate those with the SPARKID userid: RDEFINE STARTED PENZMSTR.* STDATA(USER(SPARKID) GROUP(SYS1)) RDEFINE STARTED PENZWRKR.* STDATA(USER(SPARKID) GROUP(SYS1)) RDEFINE STARTED PENZHIST.* STDATA(USER(SPARKID) GROUP(SYS1)) If creating a new Spark installation, use the book and do all that stuff. The steps above will help you locate the changes you made to IzODA environment to properly configure zSpark, as documented in the I & C Guide. IVT Regardless of whether you reused your customized IzODA Spark configuration or created a new Spark configuration, you can use the steps documented in Chapter 5. Verifying the IBM Z Platform for Apache Spark customization of the IBM Z Platform for Apache Spark Installation and Customization Guide (GI13-5809-00) User updates: Inform the user of the new product and configuration details. They will need to update their environment variables including SPARK_HOME, SPARK_CONF_DIR, logs, IBM Z Platform for Apache Spark features a different version of Spark than IzODA Spark, Spark version 3.2. To use the Spark platform, users will need to recompile their application with Spark 3.2 classes. In addition, if you are also converting from IzODA MDS to DVM, users will need to procure the appropriate DVM JDBC driver to access DVM. If any virtual tables are renamed, they will need that information to update the parameters being passed to their JDBC driver in their code. See <ref to DVM migration guide or pubs here.>","title":"Steps to migrate from IzODA Spark (Spark 2.4) to new Spark product (Spark 3.x)"},{"location":"odl/credit-risk-odl/","text":"Credit Risk Demonstration Please see the Credit Risk Example on our Github for further information.","title":"Credit Risk Demonstration"},{"location":"odl/credit-risk-odl/#credit-risk-demonstration","text":"Please see the Credit Risk Example on our Github for further information.","title":"Credit Risk Demonstration"},{"location":"odl/odl/","text":"Optimized Data Layer The Optimized Data Layer is also known as the IBM Mainframe Data Service for Apache Spark (MDS, or MDSS). It is a started task in the z/OS address space that provides a common interface to several different mainframe and off-platform data sources. The kinds of data sources that can be accessed through ODL include: IBM DB2 IBM Virtual Storage Access Method (VSAM) IBM Information Management System (IMS) Partitioned Data Sets (PDS) z/OS sequential files (including log files) ADABAS Data from other platforms: Databases - SQL and NoSQL (e.g. MongoDB, Oracle, Teradata) Streaming feeds (e.g. Twitter, Facebook) Access to these data sources is provided by: JDBC for Java and Scala applications - usually run from the Spark environment. dsdbc for Python applications running in the Anaconda environment Although ODL was originally developed to provide a JDBC interface to Scala and Java applications that use the Spark stack, it has been extended for IzODA to service Python applications through the dsdbc interface. Examples of both types of data access can be found in our Github repo. ODL includes a workstation based interface called the Data Service Studio (DSS) that is used to create, view, and change metadata describing the mapping from mainframe data sources to a relational view of the data on provided by ODL on z/OS. DSS is based on Eclipse. It can either be installed separately, or can be added to an existing Eclipse shell. For more details about the z/OS and workstation-based components of ODL and how they relate to one another, as well as details about alternative methods for virtualizing database views, please see: Chapter 2 of the IzODA Administrator's Guide Chapter 3 of the Apache Spark Redbook Installation and Customization Instructions about how to install and customize the various components of ODL are in the chapters of IzODA Installation and Customization Guide that reference MDS and the Data Service Studio. Verify Your Installation ODL is intended to be used together with one or both of the analytics stacks of IzODA. We have created a collection of small sample applications which you can run to verify that ODL and the IzODA analytics stack of your choice is installed and configured properly. Please choose one of the Install Verification Programs (IVPs) from our docs to ensure your configuration is correct. Reference Material A library of documentation is available for the IBM Open Data Analytics for z/OS offering. This can be found at the IBM Knowledge Center. The Apache Spark Implementation on IBM z/OS is another good source of information about both Spark and ODL.","title":"Overview"},{"location":"odl/odl/#optimized-data-layer","text":"The Optimized Data Layer is also known as the IBM Mainframe Data Service for Apache Spark (MDS, or MDSS). It is a started task in the z/OS address space that provides a common interface to several different mainframe and off-platform data sources. The kinds of data sources that can be accessed through ODL include: IBM DB2 IBM Virtual Storage Access Method (VSAM) IBM Information Management System (IMS) Partitioned Data Sets (PDS) z/OS sequential files (including log files) ADABAS Data from other platforms: Databases - SQL and NoSQL (e.g. MongoDB, Oracle, Teradata) Streaming feeds (e.g. Twitter, Facebook) Access to these data sources is provided by: JDBC for Java and Scala applications - usually run from the Spark environment. dsdbc for Python applications running in the Anaconda environment Although ODL was originally developed to provide a JDBC interface to Scala and Java applications that use the Spark stack, it has been extended for IzODA to service Python applications through the dsdbc interface. Examples of both types of data access can be found in our Github repo. ODL includes a workstation based interface called the Data Service Studio (DSS) that is used to create, view, and change metadata describing the mapping from mainframe data sources to a relational view of the data on provided by ODL on z/OS. DSS is based on Eclipse. It can either be installed separately, or can be added to an existing Eclipse shell. For more details about the z/OS and workstation-based components of ODL and how they relate to one another, as well as details about alternative methods for virtualizing database views, please see: Chapter 2 of the IzODA Administrator's Guide Chapter 3 of the Apache Spark Redbook","title":"Optimized Data Layer"},{"location":"odl/odl/#installation-and-customization","text":"Instructions about how to install and customize the various components of ODL are in the chapters of IzODA Installation and Customization Guide that reference MDS and the Data Service Studio.","title":"Installation and Customization"},{"location":"odl/odl/#verify-your-installation","text":"ODL is intended to be used together with one or both of the analytics stacks of IzODA. We have created a collection of small sample applications which you can run to verify that ODL and the IzODA analytics stack of your choice is installed and configured properly. Please choose one of the Install Verification Programs (IVPs) from our docs to ensure your configuration is correct.","title":"Verify Your Installation"},{"location":"odl/odl/#reference-material","text":"A library of documentation is available for the IBM Open Data Analytics for z/OS offering. This can be found at the IBM Knowledge Center. The Apache Spark Implementation on IBM z/OS is another good source of information about both Spark and ODL.","title":"Reference Material"},{"location":"spark/jupyter-remote-notebook/","text":"Jupyter with Remote Notebook on the Mainframe When Spark 2.0 was released to the public, it presented a major problem for people who use it in the mainframe environment. Spark 2.0 fully incorporates Project Tungsten , a key initiative that focuses on improving memory and CPU efficiency. One of the main features of Project Tungsten is to operate directly against binary data in order to eliminate the overhead of JVM objects and garbage collection. This new feature, however, introduced byte-order dependencies and caused issues to Spark applications running in a mixed-endian environment. For example, a Spark driver running inside a Jupyter Notebook on x86 (little-endian) talking to a Spark cluster running on z/OS (big-endian) would encounter some nasty data integrity problem. The easiest solution to this mixed-endian data integrity problem is to avoid running your Spark driver on a little-endian platform. But there are many compelling reasons to have a remote application - security, scalability, accessibility to name a few. Luckily the Jupyter Notebook extension, nb2kg , discussed below provides a solution that circumvents the byte-order issue even in a mixed-endian environment. Jupyter Kernel Gateway on the Mainframe What is Jupyter Kernel Gateway? Jupyter Kernel Gateway has several capabilities, but one of these capabilities is to offer Jupyter Notebook kernels to remote hosts. The kernels will run on the mainframe but can be executed by an x86 system. The diagram above shows how this process works. An end user would connect to a Jupyter Notebook Server with the nb2kg extension, in this case running as a Docker container. The nb2kg extension would connect to a Jupyter Kernel Gateway Server and would request access to the kernels it hosts. The Jupyter Kernel Gateway Server, which in this case runs on z/OS, would offer the kernels it hosts to the Jupyter Notebook Server that requested them, and then would run any notebooks on platform. In this reference architecture, one of the kernels the Jupyter Kernel Gateway Server would be offering is the Apache Toree kernel which offers the user an Apache Spark, Scala, Notebook environment to run under. Because the Apache Toree kernel is running on z/OS, there are no byte order issues. Setup Jupyter Notebook to Kernel Gateway w/ Apache Toree Environment Setup the Kernel Gateway and Apache Toree on z/OS Given that this example revolves around the mainframe, a lot of the work will be done on a z/OS host. The following instructions are for an iPython Kernel: To install and run the Jupyter Kernel Gateway with Apache Toree solution on z/OS, the base instructions can be found here. The steps found within the JKG2AT README will detail how to setup a Toree Kernel. Setup the Jupyter Notebook Server with nb2kg Extension Now that you have the Jupyter Kernel Gateway Server running on z/OS, you now need to setup the Jupyter Notebook Server with nb2kg extension. There are a few ways to set this up, and choosing one depends on your environment. If you wish to run the Jupyter Notebook Server on a Linux on z LPAR, follow these instructions. If you wish to run the Jupyter Notebook Server on an x86 Linux system, follow these instructions. Authors: David Rice, Jessie Yu Date: April 5th, 2017","title":"Jupyter with Remote Notebook on the Mainframe"},{"location":"spark/jupyter-remote-notebook/#jupyter-with-remote-notebook-on-the-mainframe","text":"When Spark 2.0 was released to the public, it presented a major problem for people who use it in the mainframe environment. Spark 2.0 fully incorporates Project Tungsten , a key initiative that focuses on improving memory and CPU efficiency. One of the main features of Project Tungsten is to operate directly against binary data in order to eliminate the overhead of JVM objects and garbage collection. This new feature, however, introduced byte-order dependencies and caused issues to Spark applications running in a mixed-endian environment. For example, a Spark driver running inside a Jupyter Notebook on x86 (little-endian) talking to a Spark cluster running on z/OS (big-endian) would encounter some nasty data integrity problem. The easiest solution to this mixed-endian data integrity problem is to avoid running your Spark driver on a little-endian platform. But there are many compelling reasons to have a remote application - security, scalability, accessibility to name a few. Luckily the Jupyter Notebook extension, nb2kg , discussed below provides a solution that circumvents the byte-order issue even in a mixed-endian environment.","title":"Jupyter with Remote Notebook on the Mainframe"},{"location":"spark/jupyter-remote-notebook/#jupyter-kernel-gateway-on-the-mainframe","text":"What is Jupyter Kernel Gateway? Jupyter Kernel Gateway has several capabilities, but one of these capabilities is to offer Jupyter Notebook kernels to remote hosts. The kernels will run on the mainframe but can be executed by an x86 system. The diagram above shows how this process works. An end user would connect to a Jupyter Notebook Server with the nb2kg extension, in this case running as a Docker container. The nb2kg extension would connect to a Jupyter Kernel Gateway Server and would request access to the kernels it hosts. The Jupyter Kernel Gateway Server, which in this case runs on z/OS, would offer the kernels it hosts to the Jupyter Notebook Server that requested them, and then would run any notebooks on platform. In this reference architecture, one of the kernels the Jupyter Kernel Gateway Server would be offering is the Apache Toree kernel which offers the user an Apache Spark, Scala, Notebook environment to run under. Because the Apache Toree kernel is running on z/OS, there are no byte order issues.","title":"Jupyter Kernel Gateway on the Mainframe"},{"location":"spark/jupyter-remote-notebook/#setup-jupyter-notebook-to-kernel-gateway-w-apache-toree-environment","text":"","title":"Setup Jupyter Notebook to Kernel Gateway w/ Apache Toree Environment"},{"location":"spark/jupyter-remote-notebook/#setup-the-kernel-gateway-and-apache-toree-on-zos","text":"Given that this example revolves around the mainframe, a lot of the work will be done on a z/OS host. The following instructions are for an iPython Kernel: To install and run the Jupyter Kernel Gateway with Apache Toree solution on z/OS, the base instructions can be found here. The steps found within the JKG2AT README will detail how to setup a Toree Kernel.","title":"Setup the Kernel Gateway and Apache Toree on z/OS"},{"location":"spark/jupyter-remote-notebook/#setup-the-jupyter-notebook-server-with-nb2kg-extension","text":"Now that you have the Jupyter Kernel Gateway Server running on z/OS, you now need to setup the Jupyter Notebook Server with nb2kg extension. There are a few ways to set this up, and choosing one depends on your environment. If you wish to run the Jupyter Notebook Server on a Linux on z LPAR, follow these instructions. If you wish to run the Jupyter Notebook Server on an x86 Linux system, follow these instructions. Authors: David Rice, Jessie Yu Date: April 5th, 2017","title":"Setup the Jupyter Notebook Server with nb2kg Extension"},{"location":"spark/spark/","text":"Apache Spark Apache Spark is a general purpose, high performance clustering analytics engine that allocates resources and distributes work across a set of processes and tasks. It is organized as a set core functions that underpin a collection of functional libraries. Spark was originally released through the IBM z/OS Platform for Apache Spark offering in March of 2016. Spark and Anaconda Apache Spark has no dependencies on Anaconda. However, Anaconda's package management capabilities have allowed for all of Spark's dependencies to be included in the IzODA Anaconda package library. This means that IzODA is a self-contained offering with no pre-requisites other than z/OS and Java at the proper service levels. It also means that Spark and Anaconda are effectively co-requisites, and should always be installed along with the ODL (MDS) FMID. Along with Spark's pre-requisites, those parts of the Jupyter ecosystem that run on z/OS can now be installed via conda from the JKG2AT package. Setup information about JKG2AT can be found here. Installation and Customization Refer to the IBM Knowledge Center for installation and customization instructions. The guides can be downloaded via PDF or TOC here. Further instructions about how to install and customize Apache Spark are in chapters 2 and 3 of the IzODA Installation and Customization Guide. Verify Your Installation We have created a collection of small sample applications which you can run to verify that Spark is installed and configured properly, and works with the other components of IzODA. Please choose one of the Install Verification Programs (IVPs) from the IBM Knowledge Center. Reference Material A library of documentation is available for the IBM Open Data Analytics for z/OS offering. This can be found at the IBM Knowledge Center. The Apache Spark Implementation on IBM z/OS is another good source of information about both Spark and ODL.","title":"Overview"},{"location":"spark/spark/#apache-spark","text":"Apache Spark is a general purpose, high performance clustering analytics engine that allocates resources and distributes work across a set of processes and tasks. It is organized as a set core functions that underpin a collection of functional libraries. Spark was originally released through the IBM z/OS Platform for Apache Spark offering in March of 2016.","title":"Apache Spark"},{"location":"spark/spark/#spark-and-anaconda","text":"Apache Spark has no dependencies on Anaconda. However, Anaconda's package management capabilities have allowed for all of Spark's dependencies to be included in the IzODA Anaconda package library. This means that IzODA is a self-contained offering with no pre-requisites other than z/OS and Java at the proper service levels. It also means that Spark and Anaconda are effectively co-requisites, and should always be installed along with the ODL (MDS) FMID. Along with Spark's pre-requisites, those parts of the Jupyter ecosystem that run on z/OS can now be installed via conda from the JKG2AT package. Setup information about JKG2AT can be found here.","title":"Spark and Anaconda"},{"location":"spark/spark/#installation-and-customization","text":"Refer to the IBM Knowledge Center for installation and customization instructions. The guides can be downloaded via PDF or TOC here. Further instructions about how to install and customize Apache Spark are in chapters 2 and 3 of the IzODA Installation and Customization Guide.","title":"Installation and Customization"},{"location":"spark/spark/#verify-your-installation","text":"We have created a collection of small sample applications which you can run to verify that Spark is installed and configured properly, and works with the other components of IzODA. Please choose one of the Install Verification Programs (IVPs) from the IBM Knowledge Center.","title":"Verify Your Installation"},{"location":"spark/spark/#reference-material","text":"A library of documentation is available for the IBM Open Data Analytics for z/OS offering. This can be found at the IBM Knowledge Center. The Apache Spark Implementation on IBM z/OS is another good source of information about both Spark and ODL.","title":"Reference Material"},{"location":"spark/wlm-metering/","text":"WLM Metering and Capping for IzODA Spark Introduction One of the strengths of the z Systems platform and the z/OS operating system is the option to run multiple workloads at the same time within one z/OS image or across a Parallel Sysplex. Workloads usually have different, often competing, performance and resource requirements that must be balanced to make the best use of an installation's resources, maintain the highest possible throughput, and achieve the best possible system responsiveness. The function that makes this possible is dynamic workload management, which is implemented in the workload management component of z/OS. With z/OS workload management (WLM), you define performance goals and assign a business importance to each goal. You define the goals for work in business terms, and the system decides how much resource, such as CPU or memory, to assign so as to meet the goal. WLM constantly monitors the system and adapts processing to meet the goals. In October 2016, IBM announced the intent to stage support for new capabilities in z/OS for metering and capping workloads over CPU and memory consumption. This support provides additional WLM controls that enable the system capacity planner to specify that the resource consumption of selected workloads should not exceed the specified limits. You can, for example, use WLM to ensure that all the applications or jobs submitted to a Spark cluster do not consume more than a specified real memory limit, and that they do not receive help from standard processors when the zIIP capacity is exceeded. This paper provides an overview of using WLM Metering and Capping for IBM Open Data Analytics for z/OS Spark (IzODA Spark, FMID HSPK120), and its associated considerations. Note: The IBM.Function.MemoryCapping fix category and the MEMCAP/K keyword in RETAIN identify the PTFs that enable the Metering and Capping functions. Overview of Apache Spark processes IzODA Spark is built on Apache Spark, which uses a master/worker architecture. A Spark cluster typically has multiple processes, each running in its own Java virtual machine (JVM). The following list describes the most significant processes: The master daemon allocates resources across applications. The worker daemon monitors and reports resource availability and, when directed by the master, spawns executors. The worker also monitors the liveness and resource consumption of the executors. The executor performs the actual computation and data processing for the application. The driver program runs the main function of the application and creates a SparkContext . The master and worker processes are generally lightweight. By contrast, the executors attempt to use all available system resources by default. You can use the spark-defaults.conf file, spark-env.sh file, and command line parameters to constrain the number of CPU cores and amount of memory that each executor can consume. These parameters, however, are static and require that the Spark processes be restarted for changes to take effect. (For more information about the Apache Spark configuration options, see Spark Configuration. For more information about tuning the Apache Spark cluster, see Tuning Spark. ) WLM provides a more dynamic way to manage the performance of your Spark workloads. Overview of WLM classification for IzODA Spark You specify goals for the WLM services for IzODA Spark work in the same manner as for other z/OS workloads, by associating the work with a service class. In the service class, you assign goals to the work, along with the relative importance of each goal. You can also assign limits for CPU and memory capacity to be available to a service class. To associate incoming work with a particular service class, you must also define classification rules. WLM uses classification rules to map work coming into the system to a specific service class and report class. The classification is based on work qualifiers. The first qualifier is the subsystem type that receives the work request. The subsystem type for work that is processed in z/OS UNIX (which IzODA Spark workloads are) is OMVS. You can then use the Spark process job names and/or user IDs as the secondary qualifiers to classify your Spark work. Based on your service (PTF) level, there are different ways to assign job names to your Spark processes. For more information about configuring WLM for IzODA Spark, see \u201cConfiguring z/OS workload manager for Apache Spark\u201d in IBM Open Data Analytics for z/OS Installation and Customization Guide. Figure 1. Example of WLM classification configuration Figure 1 shows a sample WLM configuration. In this example, 4 service classes are defined: ODAS1SCH, ODAS2SCH, ODAS2SCL, and ODAS2SCD. The ODAS2SCH and ODAS2SCL service classes are associated with the ODAS2RGH resource group, which allows 50 percent General Processor (GP) capacity and has no memory limit. The ODAS2SCD service class is associated with the ODAS2RGL resource group, which allows only 10 percent GP capacity and has a memory limit of 20 GB. Four classification rules are defined that classify work as follows: All processes in Spark cluster 1, whose names match the ODAS%1 pattern, are classified into the ODAS1SCH service class. Master and worker processes in Spark cluster 2, whose names match the ODAS%2* pattern, are classified into the ODAS2SCH service class. The executor for user Jill in Spark cluster 2 has a job name of ODASX002 and is classified into the ODAS2SCL service class. The executor for user Bob in Spark cluster 2 has a job name of ODASX003 and is classified into the ODAS2SCD service class. Considerations for using WLM Metering and Capping As a general rule, it is a good idea to define WLM classification rules such that Spark workload (including the MDS and Spark started procs) does not fall into the SYSOTHER service class. SYSOTHER service class work is considered discrentionary, and may suffer unexpected performance degradation. Specifying Memory Limit for a resource group A WLM resource group is a way of limiting or guaranteeing the system resource availability to one or more service classes. z/OS V2.4 (or releases with WLM APAR OA52611 applied) provides the ability to limit the physical memory, at a system level, consumed by all address spaces that belong to a resource group. Example: The following figure shows an example of creating a resource group called ODASRG with a maximum capacity of 10 percent of the LPAR share in the general processor pool and a memory limit of 20 GB: Figure 2. Example of creating a resource group If the physical memory consumption by address spaces associated with a resource group is at or near its memory limit, the system may take action against the address spaces. For instance, the system might initiate paging (within the address spaces associated with the resource group), suspend storage requesters, or, in certain situations, abnormally end (ABEND) entire address spaces. It is therefore a good practice to use the memory limit attribute as an upper limit so that, under normal conditions, resource consumption will operate well below the limit. Spark executors are generally good candidates to have their physical memory usage capped, since they typically consume a lot of memory. However, you might see performance degradation or even termination of your Spark applications if they reach their memory limit. In the case of termination, Spark applications usually observe an ABEND SEC6. Another situation that may occur is if an executor is not executing or responding for an extended period of time, such as in a heavy paging situation initiated by approaching a resource group\u2019s memory limit, the executor may be declared lost by the driver. This may prompt the spawning of a new replacement executor, and it becomes possible to have two or more executors running in the same resource group performing the same computation. In a highly competitive resource situation, the driver may declare that it cannot finish successfully and terminates. To ensure that the Spark cluster stays alive even if the system terminates executors, we recommend that you avoid placing the Spark master and worker daemons in a memory-limited resource group. We also recommend that you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_MEMORY parameter, for example, should be set to a value less than or equal to the memory limit defined for the executors' resource group. Setting appropriate values for these parameters helps Spark to dispatch work within its limits and help reduce unintended consequences. For more information about the Apache Spark configuration options for standalone mode, see Spark Standalone Mode. Specifying Honor Priority for a service class WLM APAR OA52611 also provides the ability to exclude a service class from the system-wide honor priority defaults. By setting the honor priority attribute to NO on a service class, work in that service class that is eligible to run on specialty processors (such as zIIPs) does not overflow to general-purpose CPs (GPs) when there is insufficient capacity on the specialty processors. The default is to use the IFAHONORPRIORITY and IIPHONORPRIORITY parameters that are specified in the IEAOPTxx member of parmlib. You can set the honor priority attribute to NO if you want to minimize the amount of Spark work processed by the GPs. However, give careful consideration to setting honor priority attributes to NO on service classes, especially in the case of highly utilized specialty processors. For instance, a Spark service class restricted to zIIPs may consume much of the zIIP capacity and cause other zIIP-eligible workloads, such as IBM DB2, to overflow to GPs. In addition, Spark applications, by default, fall into the default OMVS service class. If the default OMVS service class is restricted to specialty engines, other processes in the same service class, such as terminals or ssh sessions, might become unresponsive. Spark applications might also experience timeouts during driver and executor communication if they are waiting for CPU time for too long. Monitor these timeouts and adjust the timeout values in the Spark configuration accordingly. As with memory limit, we recommend you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_CORES parameter, for example, should be set to a value less than or equal to the number of processors available to the Spark executors after WLM-imposed limits. Setting the honor priority attribute to NO might also change the goals for your workload. For instance, if a Spark service class has high velocity goals and is set to use only zIIPs, these two settings might interfere with each other under certain conditions and cause an undesired state for the Spark applications and their workload priorities. See the Defining Resouce Groups section of z/OS MVS Planning: Workload Management for more information on the WLM Metering and Capping support. Authors: Jessie Yu (jessieyu@us.ibm.com), Michael Gildein (megildei@us.ibm.com), Kevin Carr (kgcarr@us.ibm.com). Date: November 27th, 2017","title":"WLM Metering and Capping for IzODA Spark"},{"location":"spark/wlm-metering/#wlm-metering-and-capping-for-izoda-spark","text":"","title":"WLM Metering and Capping for IzODA Spark"},{"location":"spark/wlm-metering/#introduction","text":"One of the strengths of the z Systems platform and the z/OS operating system is the option to run multiple workloads at the same time within one z/OS image or across a Parallel Sysplex. Workloads usually have different, often competing, performance and resource requirements that must be balanced to make the best use of an installation's resources, maintain the highest possible throughput, and achieve the best possible system responsiveness. The function that makes this possible is dynamic workload management, which is implemented in the workload management component of z/OS. With z/OS workload management (WLM), you define performance goals and assign a business importance to each goal. You define the goals for work in business terms, and the system decides how much resource, such as CPU or memory, to assign so as to meet the goal. WLM constantly monitors the system and adapts processing to meet the goals. In October 2016, IBM announced the intent to stage support for new capabilities in z/OS for metering and capping workloads over CPU and memory consumption. This support provides additional WLM controls that enable the system capacity planner to specify that the resource consumption of selected workloads should not exceed the specified limits. You can, for example, use WLM to ensure that all the applications or jobs submitted to a Spark cluster do not consume more than a specified real memory limit, and that they do not receive help from standard processors when the zIIP capacity is exceeded. This paper provides an overview of using WLM Metering and Capping for IBM Open Data Analytics for z/OS Spark (IzODA Spark, FMID HSPK120), and its associated considerations. Note: The IBM.Function.MemoryCapping fix category and the MEMCAP/K keyword in RETAIN identify the PTFs that enable the Metering and Capping functions.","title":"Introduction"},{"location":"spark/wlm-metering/#overview-of-apache-spark-processes","text":"IzODA Spark is built on Apache Spark, which uses a master/worker architecture. A Spark cluster typically has multiple processes, each running in its own Java virtual machine (JVM). The following list describes the most significant processes: The master daemon allocates resources across applications. The worker daemon monitors and reports resource availability and, when directed by the master, spawns executors. The worker also monitors the liveness and resource consumption of the executors. The executor performs the actual computation and data processing for the application. The driver program runs the main function of the application and creates a SparkContext . The master and worker processes are generally lightweight. By contrast, the executors attempt to use all available system resources by default. You can use the spark-defaults.conf file, spark-env.sh file, and command line parameters to constrain the number of CPU cores and amount of memory that each executor can consume. These parameters, however, are static and require that the Spark processes be restarted for changes to take effect. (For more information about the Apache Spark configuration options, see Spark Configuration. For more information about tuning the Apache Spark cluster, see Tuning Spark. ) WLM provides a more dynamic way to manage the performance of your Spark workloads.","title":"Overview of Apache Spark processes"},{"location":"spark/wlm-metering/#overview-of-wlm-classification-for-izoda-spark","text":"You specify goals for the WLM services for IzODA Spark work in the same manner as for other z/OS workloads, by associating the work with a service class. In the service class, you assign goals to the work, along with the relative importance of each goal. You can also assign limits for CPU and memory capacity to be available to a service class. To associate incoming work with a particular service class, you must also define classification rules. WLM uses classification rules to map work coming into the system to a specific service class and report class. The classification is based on work qualifiers. The first qualifier is the subsystem type that receives the work request. The subsystem type for work that is processed in z/OS UNIX (which IzODA Spark workloads are) is OMVS. You can then use the Spark process job names and/or user IDs as the secondary qualifiers to classify your Spark work. Based on your service (PTF) level, there are different ways to assign job names to your Spark processes. For more information about configuring WLM for IzODA Spark, see \u201cConfiguring z/OS workload manager for Apache Spark\u201d in IBM Open Data Analytics for z/OS Installation and Customization Guide. Figure 1. Example of WLM classification configuration Figure 1 shows a sample WLM configuration. In this example, 4 service classes are defined: ODAS1SCH, ODAS2SCH, ODAS2SCL, and ODAS2SCD. The ODAS2SCH and ODAS2SCL service classes are associated with the ODAS2RGH resource group, which allows 50 percent General Processor (GP) capacity and has no memory limit. The ODAS2SCD service class is associated with the ODAS2RGL resource group, which allows only 10 percent GP capacity and has a memory limit of 20 GB. Four classification rules are defined that classify work as follows: All processes in Spark cluster 1, whose names match the ODAS%1 pattern, are classified into the ODAS1SCH service class. Master and worker processes in Spark cluster 2, whose names match the ODAS%2* pattern, are classified into the ODAS2SCH service class. The executor for user Jill in Spark cluster 2 has a job name of ODASX002 and is classified into the ODAS2SCL service class. The executor for user Bob in Spark cluster 2 has a job name of ODASX003 and is classified into the ODAS2SCD service class.","title":"Overview of WLM classification for IzODA Spark"},{"location":"spark/wlm-metering/#considerations-for-using-wlm-metering-and-capping","text":"As a general rule, it is a good idea to define WLM classification rules such that Spark workload (including the MDS and Spark started procs) does not fall into the SYSOTHER service class. SYSOTHER service class work is considered discrentionary, and may suffer unexpected performance degradation.","title":"Considerations for using WLM Metering and Capping"},{"location":"spark/wlm-metering/#specifying-memory-limit-for-a-resource-group","text":"A WLM resource group is a way of limiting or guaranteeing the system resource availability to one or more service classes. z/OS V2.4 (or releases with WLM APAR OA52611 applied) provides the ability to limit the physical memory, at a system level, consumed by all address spaces that belong to a resource group. Example: The following figure shows an example of creating a resource group called ODASRG with a maximum capacity of 10 percent of the LPAR share in the general processor pool and a memory limit of 20 GB: Figure 2. Example of creating a resource group If the physical memory consumption by address spaces associated with a resource group is at or near its memory limit, the system may take action against the address spaces. For instance, the system might initiate paging (within the address spaces associated with the resource group), suspend storage requesters, or, in certain situations, abnormally end (ABEND) entire address spaces. It is therefore a good practice to use the memory limit attribute as an upper limit so that, under normal conditions, resource consumption will operate well below the limit. Spark executors are generally good candidates to have their physical memory usage capped, since they typically consume a lot of memory. However, you might see performance degradation or even termination of your Spark applications if they reach their memory limit. In the case of termination, Spark applications usually observe an ABEND SEC6. Another situation that may occur is if an executor is not executing or responding for an extended period of time, such as in a heavy paging situation initiated by approaching a resource group\u2019s memory limit, the executor may be declared lost by the driver. This may prompt the spawning of a new replacement executor, and it becomes possible to have two or more executors running in the same resource group performing the same computation. In a highly competitive resource situation, the driver may declare that it cannot finish successfully and terminates. To ensure that the Spark cluster stays alive even if the system terminates executors, we recommend that you avoid placing the Spark master and worker daemons in a memory-limited resource group. We also recommend that you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_MEMORY parameter, for example, should be set to a value less than or equal to the memory limit defined for the executors' resource group. Setting appropriate values for these parameters helps Spark to dispatch work within its limits and help reduce unintended consequences. For more information about the Apache Spark configuration options for standalone mode, see Spark Standalone Mode.","title":"Specifying Memory Limit for a resource group"},{"location":"spark/wlm-metering/#specifying-honor-priority-for-a-service-class","text":"WLM APAR OA52611 also provides the ability to exclude a service class from the system-wide honor priority defaults. By setting the honor priority attribute to NO on a service class, work in that service class that is eligible to run on specialty processors (such as zIIPs) does not overflow to general-purpose CPs (GPs) when there is insufficient capacity on the specialty processors. The default is to use the IFAHONORPRIORITY and IIPHONORPRIORITY parameters that are specified in the IEAOPTxx member of parmlib. You can set the honor priority attribute to NO if you want to minimize the amount of Spark work processed by the GPs. However, give careful consideration to setting honor priority attributes to NO on service classes, especially in the case of highly utilized specialty processors. For instance, a Spark service class restricted to zIIPs may consume much of the zIIP capacity and cause other zIIP-eligible workloads, such as IBM DB2, to overflow to GPs. In addition, Spark applications, by default, fall into the default OMVS service class. If the default OMVS service class is restricted to specialty engines, other processes in the same service class, such as terminals or ssh sessions, might become unresponsive. Spark applications might also experience timeouts during driver and executor communication if they are waiting for CPU time for too long. Monitor these timeouts and adjust the timeout values in the Spark configuration accordingly. As with memory limit, we recommend you consider your Spark configuration when using WLM to limit resources available to Spark processes. The SPARK_WORKER_CORES parameter, for example, should be set to a value less than or equal to the number of processors available to the Spark executors after WLM-imposed limits. Setting the honor priority attribute to NO might also change the goals for your workload. For instance, if a Spark service class has high velocity goals and is set to use only zIIPs, these two settings might interfere with each other under certain conditions and cause an undesired state for the Spark applications and their workload priorities. See the Defining Resouce Groups section of z/OS MVS Planning: Workload Management for more information on the WLM Metering and Capping support. Authors: Jessie Yu (jessieyu@us.ibm.com), Michael Gildein (megildei@us.ibm.com), Kevin Carr (kgcarr@us.ibm.com). Date: November 27th, 2017","title":"Specifying Honor Priority for a service class"}]}